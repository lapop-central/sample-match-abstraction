{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz, process\n",
    "#import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining specific parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = \"CL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if country == \"AR\":\n",
    "    nq_date = \"190628\"\n",
    "    geo1_nq = \"AR_provincia\"\n",
    "    geo2_nq = \"AR_departamento\"\n",
    "    geo1_ipums = 'GEO1_AR2010'\n",
    "    geo2_ipums = 'GEO2_AR2010'\n",
    "    year = '2010'\n",
    "    ipumsfile = \"../../raw/ipums/\"+country+\"/ipumsi_00015.csv\"\n",
    "    netquestfile = \"C:/Users/schadem/Box Sync/LAPOP Shared/working documents/maita/Coordination/IDB Online Crime/Matching process/Data/AR/panel/AR_netquest-panel.csv\"#\"../out/panel_country/\"+country+\"_netquest-panel.csv\"\n",
    "    dictfile = \"C:/Users/schadem/Box Sync/LAPOP Shared/working documents/maita/Coordination/IDB Online Crime/Matching process/Data/AR/panel/AR_levels.xlsx\"#\"../out/panel_country/\"+country+\"_levels.xlsx\"\n",
    "    geo2file = '../out/ipums_country/ipums_codebook_'+geo2_ipums+'.csv'\n",
    "    geo1file = '../out/ipums_country/ipums_codebook_'+geo1_ipums+'.csv'\n",
    "    centroidfile = '../out/geo/'+country+'_geo2_centroids.csv'\n",
    "\n",
    "elif country == \"BR\":\n",
    "    nq_date = \"190628\"\n",
    "    geo1_nq = \"BR_estado\"\n",
    "    geo2_nq = \"BR_cidade\"\n",
    "    geo1_ipums = 'GEO1_BR2010'\n",
    "    geo2_ipums = 'GEO2_BR2010'\n",
    "    year = '2010'\n",
    "    ipumsfile = \"../../raw/ipums/\"+country+\"/ipumsi_00026.csv\"\n",
    "    netquestfile = \"../out/panel_country/\"+country+\"_netquest-panel.csv\"\n",
    "    dictfile = \"../out/panel_country/\"+country+\"_levels.xlsx\"\n",
    "    geo2file = '../out/ipums_country/ipums_codebook_'+geo2_ipums+'.csv'\n",
    "    geo1file = '../out/ipums_country/ipums_codebook_'+geo1_ipums+'.csv'\n",
    "    centroidfile = '../out/geo/'+country+'_geo_centroids.csv'\n",
    "    \n",
    "elif country == \"CL\":\n",
    "    nq_date = \"191120\"\n",
    "    geo1_nq = \"CL_provincia\"\n",
    "    geo2_nq = \"CL_comuna\"\n",
    "    geo1_ipums = 'GEO1_CL2002'\n",
    "    geo2_ipums = 'GEO2_CL2002'\n",
    "    year = '2002'\n",
    "    ipumsfile = \"../../raw/ipums/\"+country+\"/ipumsi_00020.csv\"\n",
    "    netquestfile = \"../out/panel_country/\"+country+\"_netquest-panel.csv\"\n",
    "    dictfile = \"../out/panel_country/\"+country+\"_levels.xlsx\"\n",
    "    geo2file = '../out/ipums_country/ipums_codebook_'+geo2_ipums+'.csv'\n",
    "    geo1file = '../out/ipums_country/ipums_codebook_'+geo1_ipums+'.csv'\n",
    "    centroidfile = '../out/geo/'+country+'_geo_centroids.csv'\n",
    "    \n",
    "panelout = '../out/panel_country/'+country+'_netquest-panel_geo.csv'\n",
    "ipumsout = '../out/ipums_country/'+country+'_ipums-census_geo.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\schadem\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (22,536,537) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'../out/geo/CL_geo_centroids.csv' does not exist: b'../out/geo/CL_geo_centroids.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-6bb0460f7ae9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mipums_geo2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgeo2file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Latin1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'code'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskiprows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mipums_geo1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgeo1file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Latin1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'code'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskiprows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mgeo2_centroids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcentroidfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'latin1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'../out/geo/CL_geo_centroids.csv' does not exist: b'../out/geo/CL_geo_centroids.csv'"
     ]
    }
   ],
   "source": [
    "census = pd.read_csv(ipumsfile)\n",
    "netquest = pd.read_csv(netquestfile)\n",
    "nq_dict = pd.read_excel(dictfile)\n",
    "ipums_geo2 = pd.read_csv(geo2file,encoding='Latin1', names=['code','name'], skiprows=1)\n",
    "ipums_geo1 = pd.read_csv(geo1file,encoding='Latin1', names=['code','name'], skiprows=1)\n",
    "geo2_centroids = pd.read_csv(centroidfile, encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one for fuzzy-joining dataframes\n",
    "def fuzzy_join(df1, df2, varname, subset=False, cutoff=90):\n",
    "    '''Returns a dataframe the length of df1, with the matches on the given variable\n",
    "    \n",
    "        Assumes the var is named the same in both dataframes.\n",
    "        \n",
    "        subset is a tuple or list of length 2 whose first element is a string indicating which\n",
    "        variable in df1 has to be equal to which variable in df2 (specified by 2nd element)\n",
    "        '''\n",
    "    #figure out if there is a constraint\n",
    "    if not subset:\n",
    "        #match #1\n",
    "        ratio_matches = df1.astype(str).apply(\n",
    "                lambda d: process.extract(d[varname], \n",
    "                                             df2[varname].astype(str).drop_duplicates(),\n",
    "                                             scorer=fuzz.ratio, limit=2\n",
    "                                            ), axis=1)\n",
    "        #match #2\n",
    "        parti_matches = df1.astype(str).apply(\n",
    "                lambda d: process.extract(d[varname], \n",
    "                                             df2[varname].astype(str).drop_duplicates(),\n",
    "                                             scorer=fuzz.partial_ratio, limit=2\n",
    "                                            ), axis=1)\n",
    "        \n",
    "    else:\n",
    "        #match #1\n",
    "        ratio_matches = df1.astype(str).apply(\n",
    "                lambda d: process.extract(d[varname], \n",
    "                                             df2[varname][df2[subset[1]]==d[subset[0]]].astype(str).drop_duplicates(),\n",
    "                                             scorer=fuzz.ratio, limit=2\n",
    "                                            ), axis=1)\n",
    "        #match #2\n",
    "        parti_matches = df1.astype(str).apply(\n",
    "                lambda d: process.extract(d[varname], \n",
    "                                             df2[varname][df2[subset[1]]==d[subset[0]]].astype(str).drop_duplicates(),\n",
    "                                             scorer=fuzz.partial_ratio, limit=2\n",
    "                                            ), axis=1)    \n",
    "\n",
    "    \n",
    "    # different match cases\n",
    "    #morRatioMatch = ratio_matches.apply(lambda l: (l[0][1]==100)&(l[1][1]==100))\n",
    "    oneRatioMatch = ratio_matches.apply(lambda l: (l[0][1]>=cutoff))#&(l[1][1]<100))\n",
    "    noRatioMatch = ratio_matches.apply(lambda l: (l[0][1]<cutoff))\n",
    "\n",
    "    #morPartiMatch = parti_matches.apply(lambda l: (l[0][1]==100)&(l[1][1]==100))\n",
    "    onePartiMatch = parti_matches.apply(lambda l: (l[0][1]>=cutoff))#&(l[1][1]<100))\n",
    "    noPartiMatch = parti_matches.apply(lambda l: (l[0][1]<cutoff))\n",
    "    \n",
    "    # pick out what's better\n",
    "    matches = pd.Series([(np.nan,np.nan,np.nan)]*len(df1),index=df1.index)\n",
    "    matches.loc[oneRatioMatch] = ratio_matches.loc[oneRatioMatch].apply(lambda l: l[0])\n",
    "    matches.loc[(noRatioMatch&onePartiMatch)] = parti_matches.loc[(noRatioMatch&onePartiMatch)]\\\n",
    "        .apply(lambda l: l[0])\n",
    "    matches.loc[(noRatioMatch&noPartiMatch)] = parti_matches.loc[(noRatioMatch&noPartiMatch)]\\\n",
    "        .apply(lambda l: l[0])\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df['name'] = matches.apply(lambda l: l[0])\n",
    "    df['score'] = matches.apply(lambda l: l[1])\n",
    "    df['index'] = matches.apply(lambda l: l[2])\n",
    "    df['parti_matches'] = parti_matches\n",
    "    df['ratio_matches'] = ratio_matches\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up helper structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\schadem\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\schadem\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\schadem\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "nq_geo1 = nq_dict[nq_dict.Variable==geo1_nq]\n",
    "nq_geo1.columns = [\"Variable\",geo1_nq,geo1_nq+\"_name\"]\n",
    "nq_geo1 = nq_geo1[[geo1_nq,geo1_nq+\"_name\"]]\n",
    "\n",
    "nq_geo2 = nq_dict[nq_dict.Variable==geo2_nq]\n",
    "nq_geo2.columns = [\"Variable\",geo2_nq,geo2_nq+\"_name\"]\n",
    "nq_geo2 = nq_geo2[[geo2_nq,geo2_nq+\"_name\"]]\n",
    "\n",
    "# setting up unique DF for geographies\n",
    "ipums_geodf = census[[geo1_ipums,geo2_ipums]]\n",
    "ipums_geodf.columns = ['geo1_code','geo2_code']\n",
    "ipums_geodf.drop_duplicates(subset=['geo1_code','geo2_code'],inplace=True)\n",
    "\n",
    "ipums_geodf['geo1_name'] = ipums_geodf.merge(ipums_geo1, \n",
    "                                             how='left', \n",
    "                                             left_on = \"geo1_code\", \n",
    "                                             right_on = 'code', \n",
    "                                             copy=False)['name'].values\n",
    "\n",
    "ipums_geodf['geo2_name'] = ipums_geodf.merge(ipums_geo2, \n",
    "                                             how='left', \n",
    "                                             left_on = \"geo2_code\", \n",
    "                                             right_on = 'code', \n",
    "                                             copy=False)['name'].values\n",
    "\n",
    "# Melting doubled-up geographies\n",
    "ipums_geodf = ipums_geodf.geo2_name.str.split(',').apply(pd.Series) \\\n",
    "    .merge(ipums_geodf, right_index = True, left_index = True) \\\n",
    "    .drop([\"geo2_name\"], axis = 1) \\\n",
    "    .melt(id_vars = [k for k in ipums_geodf.columns if not (type(k)==int)|(k=='geo2_name')], value_name = \"geo2_name\") \\\n",
    "    .drop(\"variable\", axis = 1) \\\n",
    "    .dropna(subset=['geo2_name'])\n",
    "ipums_geodf.geo2_name = ipums_geodf.geo2_name.str.strip()\n",
    "\n",
    "ipums_geodf = ipums_geodf.geo1_name.str.split(',').apply(pd.Series) \\\n",
    "    .merge(ipums_geodf, right_index = True, left_index = True) \\\n",
    "    .drop([\"geo1_name\"], axis = 1) \\\n",
    "    .melt(id_vars = [k for k in ipums_geodf.columns if not (type(k)==int)|(k=='geo1_name')], value_name = \"geo1_name\") \\\n",
    "    .drop(\"variable\", axis = 1) \\\n",
    "    .dropna(subset=['geo1_name'])\n",
    "ipums_geodf.geo1_name = ipums_geodf.geo1_name.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2816 2817 2818 2819 2820 2821 2822 2823 2824 2825 2826 2827 2828 2829\n",
      " 2830 2831 2832 2833 2834 2835 2836 2837 2838 2839 2840 2841 2842]\n",
      "[26. 18. 19. 11.  7. 23. 10. 24.  9. 21.  6.  5. 15. 16. 20. 13.  8.  2.\n",
      "  4. 12. 27. 22. 14. 25.  3. 17.  1. nan]\n"
     ]
    }
   ],
   "source": [
    "print(nq_geo1.BR_estado.unique())\n",
    "print(nq_geodf.geo1_code.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Country-specific pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "if country==\"AR\":\n",
    "    # accounting for the CABA mess\n",
    "    ipums_geodf.loc[ipums_geodf.geo1_name==\"City of Buenos Aires\",\n",
    "             'geo1_name'] = 'CABA'\n",
    "    ipums_geodf.loc[(ipums_geodf.geo1_name==\"CABA\"),'geo2_name'] = 'CABA'\n",
    "    caba_codes = ipums_geodf[ipums_geodf.geo1_name==\"CABA\"].geo2_code\n",
    "    # dropping CABA-related duplicates\n",
    "    ipums_geodf.drop_duplicates(subset=[\"geo1_name\",\"geo2_name\"], inplace=True)\n",
    "    # make sure there is only one CABA-code left\n",
    "    #print(len(ipums_geodf[ipums_geodf.geo1_name==\"CABA\"].geo2_code.values[0])==1)\n",
    "    census.loc[census[geo2_ipums].isin(caba_codes),geo2_ipums] = \\\n",
    "    len(census.loc[census[geo2_ipums].isin(caba_codes),geo2_ipums])*[ipums_geodf[ipums_geodf.geo1_name==\"CABA\"].geo2_code[0]]\n",
    "    \n",
    "    # give CABA locations without geo2 a specific code: 999\n",
    "    netquest.loc[netquest[geo2_nq].isna()&(netquest[geo1_nq]==1),geo2_nq] = 999\n",
    "    nq_geodf = netquest.merge(nq_geo2, on=geo2_nq, how='left')\\\n",
    "                       .merge(nq_geo1, on=geo1_nq, how='left')[[geo1_nq,geo1_nq+'_name',geo2_nq,geo2_nq+'_name']]\n",
    "    nq_geodf.columns = ['geo1_code','geo1_name','geo2_code','geo2_name']\n",
    "    nq_geodf.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Fixing CABA not having Dept.\n",
    "    # Where province is CABA and no departamento exists, call these \"CABA\"\n",
    "    nq_geodf.loc[nq_geodf.geo1_name==\"Ciudad Autónoma de Buenos Aires\",\n",
    "                 'geo1_name'] = 'CABA'\n",
    "    nq_geodf = nq_geodf[(nq_geodf['geo1_code'].notna()) & (nq_geodf['geo2_code'].notna())] \n",
    "\n",
    "if country==\"BR\":\n",
    "    nq_geodf = netquest.merge(nq_geo2, on=geo2_nq, how='left')\\\n",
    "                       .merge(nq_geo1, on=geo1_nq, how='left')[[geo1_nq,geo1_nq+'_name',geo2_nq,geo2_nq+'_name']]\n",
    "    nq_geodf.columns = ['geo1_code','geo1_name','geo2_code','geo2_name']\n",
    "\n",
    "    nq_geodf.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geo1_code</th>\n",
       "      <th>geo1_name</th>\n",
       "      <th>geo2_code</th>\n",
       "      <th>geo2_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3298.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3961.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3129.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3111.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3173.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558953</th>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>369.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560797</th>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>605.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561604</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565461</th>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>728.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566175</th>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2212.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5479 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        geo1_code geo1_name  geo2_code geo2_name\n",
       "0            26.0       NaN     3298.0       NaN\n",
       "1            18.0       NaN     3961.0       NaN\n",
       "2            19.0       NaN     3129.0       NaN\n",
       "3            19.0       NaN     3111.0       NaN\n",
       "4            26.0       NaN     3173.0       NaN\n",
       "...           ...       ...        ...       ...\n",
       "558953       27.0       NaN      369.0       NaN\n",
       "560797       10.0       NaN      605.0       NaN\n",
       "561604        3.0       NaN       98.0       NaN\n",
       "565461       17.0       NaN      728.0       NaN\n",
       "566175       11.0       NaN     2212.0       NaN\n",
       "\n",
       "[5479 rows x 4 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nq_geodf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Country-independent pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the duplicates in nq_geodf.geo2_code, and keeping the combinations that occur most often (which are quite clearly the good ones).\n",
    "nq_geodf['count'] = nq_geodf.apply(\n",
    "    lambda r: sum((netquest[geo1_nq]==r['geo1_code'])&\n",
    "                  (netquest[geo2_nq]==r['geo2_code'])),\n",
    "                                   axis=1)\n",
    "nq_geodf = nq_geodf.sort_values(['geo2_code','count']) \\\n",
    "    [~(nq_geodf.sort_values(['geo2_code','count']).duplicated('geo2_code',keep='last'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More country-specific processing--manual fixes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "if country==\"AR\":\n",
    "    nq_geodf.loc[nq_geodf.geo2_code==999, 'geo2_name'] = \"CABA\"\n",
    "\n",
    "    ipums_geodf.loc[ipums_geodf.geo1_name.str.contains(\"Buenos Aires province\"),\n",
    "                 'geo1_name'] = \"Buenos Aires\"\n",
    "    \n",
    "    ipums_geodf.loc[ipums_geodf.geo2_name.str.contains(\"Puan\"),\n",
    "                    'geo2_name'] = \"Puán\"\n",
    "    ipums_geodf.loc[(ipums_geodf.geo2_name.str.contains(\"General San Martín\"))\\\n",
    "                    &(ipums_geodf.geo1_name==\"Buenos Aires\"),\n",
    "                    'geo2_name'] = \"Ciudad Libertador San Martín\"\n",
    "    ipums_geodf.loc[(ipums_geodf.geo2_name.str.contains(\"La Capital\"))\\\n",
    "                    &(ipums_geodf.geo1_name==\"San Luis\"),\n",
    "                    'geo2_name'] = \"Juan Martín de Pueyrredón\"\n",
    "    ipums_geodf.loc[(ipums_geodf.geo2_name==\"Maipú\")\\\n",
    "                    &(ipums_geodf.geo2_code==6050),\n",
    "                    'geo2_name'] = \"Marcos Paz\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    ipums_geodf.loc[(ipums_geodf.geo2_name.str.contains(\"Chascomus\")),\n",
    "                    'geo2_name'] = \"Chascomús\"\n",
    "    ipums_geodf.loc[(ipums_geodf.geo2_name.str.contains(\"Jose C. Paz\")),\n",
    "                    'geo2_name'] = \"José C. Paz\"\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name.str.contains(\"Paso de Indios\"))\\\n",
    "                    &(nq_geodf.geo1_name==\"Chubut\"),\n",
    "                    'geo2_name'] = \"Paso de los Indios\"\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name.str.contains(\"Coronel de Marina Leonardo Rosales\"))\\\n",
    "                    &(nq_geodf.geo1_name==\"Buenos Aires\"),\n",
    "                    'geo2_name'] = \"Coronel de Marine L. Rosales\"\n",
    "    ipums_geodf.loc[(ipums_geodf.geo2_name.str.contains(\"Veinticinco de Mayo\"))\\\n",
    "                    &(ipums_geodf.geo1_name==\"Buenos Aires\"),\n",
    "                    'geo2_name'] = \"25 de Mayo\"\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name.str.contains(\"Pueyrredón\"))\\\n",
    "                    &(nq_geodf.geo1_name==\"Buenos Aires\"),\n",
    "                    'geo2_name'] = \"General Pueyrredón\"\n",
    "    \n",
    "elif country==\"BR\":\n",
    "    ipums_geodf.loc[ipums_geodf.geo1_name.str.contains(\"Federal District\"),\n",
    "                'geo1_name'] = \"Distrito Federal\"\n",
    "\n",
    "\n",
    "    ipums_geodf.loc[(ipums_geodf.geo2_name.str.contains(\"Santarém\"))& \\\n",
    "                    (ipums_geodf.geo1_name.str.contains(\"Paraíba\")),\n",
    "                    'geo2_name'] = \"Joca Claudino\"\n",
    "\n",
    "    ipums_geodf.loc[(ipums_geodf.geo2_name.str.contains(\"Presidente Juscelino\"))& \\\n",
    "                    (ipums_geodf.geo1_name.str.contains(\"Rio Grande do Norte\")),\n",
    "                    'geo2_name'] = \"Serra Caiada\"\n",
    "\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name.str.contains(\"Bonfim\"))& \\\n",
    "                    (nq_geodf.geo1_name.str.contains(\"Rio de Janeiro\")),\n",
    "                    'geo1_name'] = \"Minas Gerais\"\n",
    "\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name.str.contains(\"Iracema\"))& \\\n",
    "                    (nq_geodf.geo1_name.str.contains(\"Santa Catarina\")),\n",
    "                    'geo2_name'] = \"Itaiópolis\"\n",
    "\n",
    "\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Pescaria Brava\",'geo2_name'] = 'Laguna'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Balneário Rincão\",'geo2_name'] = 'Içara'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Pinto Bandeira\",'geo2_name'] = 'Bento Gonçalves'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Paraíso das Águas\",'geo2_name'] = 'Água Clara'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Açailândia\",'geo1_name'] = 'Maranhão'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Mariana\",'geo1_name'] = 'Minas Gerais'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Fortaleza\",'geo1_name'] = 'Ceará'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name.isin([\"Leme\",\n",
    "                                          \"São Paulo\",\n",
    "                                          'Itapevi',\n",
    "                                          \"Franca\",\n",
    "                                          \"Presidente Prudente\",\n",
    "                                          \"Suzano\",\n",
    "                                          \"Ourinhos\"\n",
    "                                         ]),'geo1_name'] = 'São Paulo'\n",
    "\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name==\"Praia Grande\") &\\\n",
    "                 ((nq_geodf.geo1_name==\"Acre\")|(nq_geodf.geo1_name==\"Maranhão\")),\n",
    "                 'geo1_name'] = 'São Paulo'\n",
    "\n",
    "    nq_geodf.loc[nq_geodf.geo1_name==\"Brasília\",'geo1_name'] = 'Distrito Federal'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Brasília\",'geo1_name'] = 'Distrito Federal'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Tupanatinga\",'geo1_name'] = 'Pernambuco'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Macaé\",'geo1_name'] = 'Rio de Janeiro'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Queimados\",'geo1_name'] = 'Rio de Janeiro'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"São Miguel do Iguaçu\",'geo1_name'] = 'Paraná'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Empty DataFrame\n",
      "Columns: [geo1_name, geo1_code, geo2_name, geo2_code, geo2_match_name, geo2_match_score]\n",
      "Index: []\n",
      "AR: Decide at what level to cut off matches: 50\n"
     ]
    }
   ],
   "source": [
    "# The fuzzy join\n",
    "nq_geodf[['geo1_match_name','geo1_match_score','geo1_match_index']] \\\n",
    "= fuzzy_join(nq_geodf, ipums_geodf, 'geo1_name')[['name','score','index']]\n",
    "nq_geodf[['geo2_match_name','geo2_match_score','geo2_match_index']] \\\n",
    "= fuzzy_join(nq_geodf, ipums_geodf, 'geo2_name', \n",
    "             subset=['geo1_match_name','geo1_name']\n",
    "            )[['name','score','index']]\n",
    "\n",
    "# confirmation of results\n",
    "print(\"Are there no more duplicates?\")\n",
    "print(\n",
    "    len(nq_geodf[nq_geodf.duplicated('geo2_code', keep=False)].sort_values('geo2_code')\\\n",
    "    [[\"geo1_code\",\"geo2_code\",\n",
    "      \"geo1_name\",\"geo1_match_name\",\n",
    "      \"geo2_name\",\"geo2_match_name\",\n",
    "      \"count\"]]\n",
    "    )==0\n",
    ")      \n",
    "    \n",
    "# determination of cutoff\n",
    "print(\"Showing problematic geographies...\")\n",
    "print(\n",
    "      nq_geodf[(nq_geodf.geo2_match_score<80)][['geo1_name',\n",
    "                                          'geo1_code',\n",
    "#                                         'geo1_match_name',\n",
    "                                        'geo2_name',\n",
    "                                        'geo2_code',\n",
    "                                        'geo2_match_name',\n",
    "                                        'geo2_match_score'\n",
    "                                       ]\n",
    "                ].sort_values('geo1_name'))\n",
    "cutoff = input(country + \": Decide at what level to cut off matches: \")\n",
    "has_ipums_geo = nq_geodf.geo2_match_score>int(cutoff )\n",
    "\n",
    "# attaching geography codes\n",
    "nq_geodf['IPUMS_geo2_code'] = np.nan\n",
    "\n",
    "nq_geodf.loc[has_ipums_geo,'IPUMS_geo2_code'] = nq_geodf[has_ipums_geo]\\\n",
    "                            .geo2_match_index\\\n",
    "                            .astype(int)\\\n",
    "                            .apply(\n",
    "                                lambda i: ipums_geodf.loc[i,'geo2_code']\n",
    "                            .astype(int)\n",
    ")\n",
    "                            \n",
    "# Merge centroids onto NQ geometries:\n",
    "nq_geodf_merged = nq_geodf.merge(geo2_centroids[['ADMIN_NAME','Y','X','IPUM'+year]], \n",
    "               left_on='IPUMS_geo2_code',\n",
    "               right_on=\"IPUM\"+year,\n",
    "               how='left'\n",
    "              ).drop('IPUM'+year, axis=1)\n",
    "#Merge NQ geometries onto NQ data:\n",
    "panel_geo = netquest.merge(nq_geodf_merged[['X','Y','geo2_code']],\n",
    "               left_on=geo2_nq,\n",
    "               right_on='geo2_code',\n",
    "               how='left'\n",
    "              )\n",
    "#Merge census geometries onto census data\n",
    "census_geo = census.merge(geo2_centroids[['ADMIN_NAME','X',\"Y\",'IPUM'+year]],\n",
    "                          left_on = geo2_ipums,\n",
    "                          right_on='IPUM'+year,\n",
    "                          how='left'\n",
    "                         ).drop('IPUM'+year,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if panel_geo.shape[0]!=netquest.shape[0]:\n",
    "    print(\"Problem with panel shape match\")\n",
    "if census_geo.shape[0]!=census.shape[0]:\n",
    "    print(\"Problem with census shape match\")\n",
    "panel_geo.loc[:,[not(\"Unnamed\" in k) for k in panel_geo.columns]].to_csv(panelout)\n",
    "census_geo.loc[:,[not(\"Unnamed\" in k) for k in census_geo.columns]].to_csv(ipumsout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
