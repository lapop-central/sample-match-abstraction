{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz, process\n",
    "#import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining specific parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = \"AR\"\n",
    "nq_date = \"190628\"\n",
    "geo1_nq = \"AR_provincia\"\n",
    "geo2_nq = \"AR_departamento\"\n",
    "geo1_ipums = 'GEO1_AR2010'\n",
    "geo2_ipums = 'GEO2_AR2010'\n",
    "year = '2010'\n",
    "ipumsfile = \"../../raw/ipums/\"+country+\"/ipumsi_00015.csv\"\n",
    "netquestfile = \"../out/panel_country/\"+country+\"_netquest-panel.csv\"\n",
    "dictfile = \"../out/panel_country/\"+country+\"_levels.xlsx\"\n",
    "geo2file = '../out/ipums_country/ipums_codebook_'+geo2_ipums+'.csv'\n",
    "geo1file = '../out/ipums_country/ipums_codebook_'+geo1_ipums+'.csv'\n",
    "centroidfile = '../out/geo/'+country+'_geo2_centroids.csv'\n",
    "panelout = '../out/panel_country/'+country+'_netquest-panel_geo.csv'\n",
    "ipumsout = '../out/ipums_country/'+country+'_ipums-census_geo.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\schadem\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "census = pd.read_csv(ipumsfile)\n",
    "netquest = pd.read_csv(netquestfile)\n",
    "nq_dict = pd.read_excel(dictfile)\n",
    "ipums_geo2 = pd.read_csv(geo2file,encoding='Latin1', names=['code','name'], skiprows=1)\n",
    "ipums_geo1 = pd.read_csv(geo1file,encoding='Latin1', names=['code','name'], skiprows=1)\n",
    "geo2_centroids = pd.read_csv(centroidfile, encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one for fuzzy-joining dataframes\n",
    "def fuzzy_join(df1, df2, varname, subset=False, cutoff=90):\n",
    "    '''Returns a dataframe the length of df1, with the matches on the given variable\n",
    "    \n",
    "        Assumes the var is named the same in both dataframes.\n",
    "        \n",
    "        subset is a tuple or list of length 2 whose first element is a string indicating which\n",
    "        variable in df1 has to be equal to which variable in df2 (specified by 2nd element)\n",
    "        '''\n",
    "    #figure out if there is a constraint\n",
    "    if not subset:\n",
    "        #match #1\n",
    "        ratio_matches = df1.astype(str).apply(\n",
    "                lambda d: process.extract(d[varname], \n",
    "                                             df2[varname].astype(str).drop_duplicates(),\n",
    "                                             scorer=fuzz.ratio, limit=2\n",
    "                                            ), axis=1)\n",
    "        #match #2\n",
    "        parti_matches = df1.astype(str).apply(\n",
    "                lambda d: process.extract(d[varname], \n",
    "                                             df2[varname].astype(str).drop_duplicates(),\n",
    "                                             scorer=fuzz.partial_ratio, limit=2\n",
    "                                            ), axis=1)\n",
    "        \n",
    "    else:\n",
    "        #match #1\n",
    "        ratio_matches = df1.astype(str).apply(\n",
    "                lambda d: process.extract(d[varname], \n",
    "                                             df2[varname][df2[subset[1]]==d[subset[0]]].astype(str).drop_duplicates(),\n",
    "                                             scorer=fuzz.ratio, limit=2\n",
    "                                            ), axis=1)\n",
    "        #match #2\n",
    "        parti_matches = df1.astype(str).apply(\n",
    "                lambda d: process.extract(d[varname], \n",
    "                                             df2[varname][df2[subset[1]]==d[subset[0]]].astype(str).drop_duplicates(),\n",
    "                                             scorer=fuzz.partial_ratio, limit=2\n",
    "                                            ), axis=1)    \n",
    "\n",
    "    \n",
    "    # different match cases\n",
    "    #morRatioMatch = ratio_matches.apply(lambda l: (l[0][1]==100)&(l[1][1]==100))\n",
    "    oneRatioMatch = ratio_matches.apply(lambda l: (l[0][1]>=cutoff))#&(l[1][1]<100))\n",
    "    noRatioMatch = ratio_matches.apply(lambda l: (l[0][1]<cutoff))\n",
    "\n",
    "    #morPartiMatch = parti_matches.apply(lambda l: (l[0][1]==100)&(l[1][1]==100))\n",
    "    onePartiMatch = parti_matches.apply(lambda l: (l[0][1]>=cutoff))#&(l[1][1]<100))\n",
    "    noPartiMatch = parti_matches.apply(lambda l: (l[0][1]<cutoff))\n",
    "    \n",
    "    # pick out what's better\n",
    "    matches = pd.Series([(np.nan,np.nan,np.nan)]*len(df1),index=df1.index)\n",
    "    matches.loc[oneRatioMatch] = ratio_matches.loc[oneRatioMatch].apply(lambda l: l[0])\n",
    "    matches.loc[(noRatioMatch&onePartiMatch)] = parti_matches.loc[(noRatioMatch&onePartiMatch)]\\\n",
    "        .apply(lambda l: l[0])\n",
    "    matches.loc[(noRatioMatch&noPartiMatch)] = parti_matches.loc[(noRatioMatch&noPartiMatch)]\\\n",
    "        .apply(lambda l: l[0])\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df['name'] = matches.apply(lambda l: l[0])\n",
    "    df['score'] = matches.apply(lambda l: l[1])\n",
    "    df['index'] = matches.apply(lambda l: l[2])\n",
    "    df['parti_matches'] = parti_matches\n",
    "    df['ratio_matches'] = ratio_matches\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up helper structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\schadem\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\schadem\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\schadem\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "nq_geo1 = nq_dict[nq_dict.Variable==geo1_nq]\n",
    "nq_geo1.columns = [\"Variable\",geo1_nq,geo1_nq+\"_name\"]\n",
    "nq_geo1 = nq_geo1[[geo1_nq,geo1_nq+\"_name\"]]\n",
    "\n",
    "nq_geo2 = nq_dict[nq_dict.Variable==geo2_nq]\n",
    "nq_geo2.columns = [\"Variable\",geo2_nq,geo2_nq+\"_name\"]\n",
    "nq_geo2 = nq_geo2[[geo2_nq,geo2_nq+\"_name\"]]\n",
    "\n",
    "# setting up unique DF for geographies\n",
    "ipums_geodf = census[[geo1_ipums,geo2_ipums]]\n",
    "ipums_geodf.columns = ['geo1_code','geo2_code']\n",
    "ipums_geodf.drop_duplicates(subset=['geo1_code','geo2_code'],inplace=True)\n",
    "\n",
    "ipums_geodf['geo1_name'] = ipums_geodf.merge(ipums_geo1, \n",
    "                                             how='left', \n",
    "                                             left_on = \"geo1_code\", \n",
    "                                             right_on = 'code', \n",
    "                                             copy=False)['name'].values\n",
    "\n",
    "ipums_geodf['geo2_name'] = ipums_geodf.merge(ipums_geo2, \n",
    "                                             how='left', \n",
    "                                             left_on = \"geo2_code\", \n",
    "                                             right_on = 'code', \n",
    "                                             copy=False)['name'].values\n",
    "\n",
    "# Melting doubled-up geographies\n",
    "ipums_geodf = ipums_geodf.geo2_name.str.split(',').apply(pd.Series) \\\n",
    "    .merge(ipums_geodf, right_index = True, left_index = True) \\\n",
    "    .drop([\"geo2_name\"], axis = 1) \\\n",
    "    .melt(id_vars = [k for k in ipums_geodf.columns if not (type(k)==int)|(k=='geo2_name')], value_name = \"geo2_name\") \\\n",
    "    .drop(\"variable\", axis = 1) \\\n",
    "    .dropna(subset=['geo2_name'])\n",
    "ipums_geodf.geo2_name = ipums_geodf.geo2_name.str.strip()\n",
    "\n",
    "ipums_geodf = ipums_geodf.geo1_name.str.split(',').apply(pd.Series) \\\n",
    "    .merge(ipums_geodf, right_index = True, left_index = True) \\\n",
    "    .drop([\"geo1_name\"], axis = 1) \\\n",
    "    .melt(id_vars = [k for k in ipums_geodf.columns if not (type(k)==int)|(k=='geo1_name')], value_name = \"geo1_name\") \\\n",
    "    .drop(\"variable\", axis = 1) \\\n",
    "    .dropna(subset=['geo1_name'])\n",
    "ipums_geodf.geo1_name = ipums_geodf.geo1_name.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Country-specific pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if country==\"AR\":\n",
    "    # accounting for the CABA mess\n",
    "    ipums_geodf.loc[ipums_geodf.geo1_name==\"City of Buenos Aires\",\n",
    "             'geo1_name'] = 'CABA'\n",
    "    ipums_geodf.loc[(ipums_geodf.geo1_name==\"CABA\"),'geo2_name'] = 'CABA'\n",
    "    caba_codes = ipums_geodf[ipums_geodf.geo1_name==\"CABA\"].geo2_code\n",
    "    # dropping CABA-related duplicates\n",
    "    ipums_geodf.drop_duplicates(subset=[\"geo1_name\",\"geo2_name\"], inplace=True)\n",
    "    # make sure there is only one CABA-code left\n",
    "    #print(len(ipums_geodf[ipums_geodf.geo1_name==\"CABA\"].geo2_code.values[0])==1)\n",
    "    census.loc[census[geo2_ipums].isin(caba_codes),geo2_ipums] = \\\n",
    "    len(census.loc[census[geo2_ipums].isin(caba_codes),geo2_ipums])*[ipums_geodf[ipums_geodf.geo1_name==\"CABA\"].geo2_code[0]]\n",
    "    \n",
    "    # give CABA locations without geo2 a specific code: 999\n",
    "    netquest.loc[netquest[geo2_nq].isna()&(netquest[geo1_nq]==1),geo2_nq] = 999\n",
    "    nq_geodf = netquest.merge(nq_geo2, on=geo2_nq, how='left')\\\n",
    "                       .merge(nq_geo1, on=geo1_nq, how='left')[[geo1_nq,geo1_nq+'_name',geo2_nq,geo2_nq+'_name']]\n",
    "    nq_geodf.columns = ['geo1_code','geo1_name','geo2_code','geo2_name']\n",
    "    nq_geodf.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Fixing CABA not having Dept.\n",
    "    # Where province is CABA and no departamento exists, call these \"CABA\"\n",
    "    nq_geodf.loc[nq_geodf.geo1_name==\"Ciudad Autónoma de Buenos Aires\",\n",
    "                 'geo1_name'] = 'CABA'\n",
    "    nq_geodf = nq_geodf[(nq_geodf['geo1_code'].notna()) & (nq_geodf['geo2_code'].notna())] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Country-independent pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the duplicates in nq_geodf.geo2_code, and keeping the combinations that occur most often (which are quite clearly the good ones).\n",
    "nq_geodf['count'] = nq_geodf.apply(\n",
    "    lambda r: sum((netquest[geo1_nq]==r['geo1_code'])&\n",
    "                  (netquest[geo2_nq]==r['geo2_code'])),\n",
    "                                   axis=1)\n",
    "nq_geodf = nq_geodf.sort_values(['geo2_code','count']) \\\n",
    "    [~(nq_geodf.sort_values(['geo2_code','count']).duplicated('geo2_code',keep='last'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netquest[netquest.AR_departamento==301].AR_provincia.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More country-specific processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if country==\"AR\":\n",
    "    nq_geodf.loc[nq_geodf.geo2_code==999, 'geo2_name'] = \"CABA\"\n",
    "\n",
    "    ipums_geodf.loc[ipums_geodf.geo1_name.str.contains(\"Buenos Aires province\"),\n",
    "                 'geo1_name'] = \"Buenos Aires\"\n",
    "    \n",
    "    ipums_geodf.loc[ipums_geodf.geo2_name.str.contains(\"Puan\"),\n",
    "                    'geo2_name'] = \"Puán\"\n",
    "    ipums_geodf.loc[(ipums_geodf.geo2_name.str.contains(\"General San Martín\"))\\\n",
    "                    &(ipums_geodf.geo1_name==\"Buenos Aires\"),\n",
    "                    'geo2_name'] = \"Ciudad Libertador San Martín\"\n",
    "    ipums_geodf.loc[(ipums_geodf.geo2_name.str.contains(\"La Capital\"))\\\n",
    "                    &(ipums_geodf.geo1_name==\"San Luis\"),\n",
    "                    'geo2_name'] = \"Juan Martín de Pueyrredón\"\n",
    "    ipums_geodf.loc[(ipums_geodf.geo2_name==\"Maipú\")\\\n",
    "                    &(ipums_geodf.geo2_code==6050),\n",
    "                    'geo2_name'] = \"Marcos Paz\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    ipums_geodf.loc[(ipums_geodf.geo2_name.str.contains(\"Chascomus\")),\n",
    "                    'geo2_name'] = \"Chascomús\"\n",
    "    ipums_geodf.loc[(ipums_geodf.geo2_name.str.contains(\"Jose C. Paz\")),\n",
    "                    'geo2_name'] = \"José C. Paz\"\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name.str.contains(\"Paso de Indios\"))\\\n",
    "                    &(nq_geodf.geo1_name==\"Chubut\"),\n",
    "                    'geo2_name'] = \"Paso de los Indios\"\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name.str.contains(\"Coronel de Marina Leonardo Rosales\"))\\\n",
    "                    &(nq_geodf.geo1_name==\"Buenos Aires\"),\n",
    "                    'geo2_name'] = \"Coronel de Marine L. Rosales\"\n",
    "    ipums_geodf.loc[(ipums_geodf.geo2_name.str.contains(\"Veinticinco de Mayo\"))\\\n",
    "                    &(ipums_geodf.geo1_name==\"Buenos Aires\"),\n",
    "                    'geo2_name'] = \"25 de Mayo\"\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name.str.contains(\"Pueyrredón\"))\\\n",
    "                    &(nq_geodf.geo1_name==\"Buenos Aires\"),\n",
    "                    'geo2_name'] = \"General Pueyrredón\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "       geo1_name  geo1_code             geo2_name  geo2_code  \\\n",
      "1199   Catamarca       20.0  San Antonio de Areco      113.0   \n",
      "420        Chaco       10.0               Lavalle      309.0   \n",
      "10560      Chaco       10.0      Berón de Astrada      298.0   \n",
      "37604      Chaco       10.0               Capital      299.0   \n",
      "5697       Chaco       10.0         Curuzu Cuatia      301.0   \n",
      "...          ...        ...                   ...        ...   \n",
      "774      Tucumán        6.0          Las Colonias      170.0   \n",
      "182      Tucumán        6.0         San Cristóbal      172.0   \n",
      "12298    Tucumán        6.0           San Lorenzo      176.0   \n",
      "525      Tucumán        6.0            9 de Julio      161.0   \n",
      "22       Tucumán        6.0         Adolfo Alsina        3.0   \n",
      "\n",
      "             geo2_match_name  geo2_match_score  \n",
      "1199                 Ancasti                57  \n",
      "420     Comandante Fernandez                43  \n",
      "10560             2 de Abril                60  \n",
      "37604        Sargento Cabral                57  \n",
      "5697                   Maipú                40  \n",
      "...                      ...               ...  \n",
      "774                 La Cocha                62  \n",
      "182                  Capital                57  \n",
      "12298  Juan Bautista Alberdi                45  \n",
      "525           Tafí del Valle                50  \n",
      "22                    Leales                50  \n",
      "\n",
      "[287 rows x 6 columns]\n",
      "AR: Decide at what level to cut off matches: \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid comparison between dtype=int64 and str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-d73c4f28ed92>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m                 ].sort_values('geo1_name'))\n\u001b[0;32m     30\u001b[0m \u001b[0mcutoff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"AR: Decide at what level to cut off matches: \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mhas_ipums_geo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnq_geodf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeo2_match_score\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0mcutoff\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m# attaching geography codes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, other, axis)\u001b[0m\n\u001b[0;32m   1227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1228\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1229\u001b[1;33m                 \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mna_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1230\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1231\u001b[0m                 raise TypeError(\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\__init__.py\u001b[0m in \u001b[0;36mna_op\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1115\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1117\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0minvalid_comparison\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\__init__.py\u001b[0m in \u001b[0;36minvalid_comparison\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    495\u001b[0m         raise TypeError(\n\u001b[0;32m    496\u001b[0m             \"Invalid comparison between dtype={dtype} and {typ}\".format(\n\u001b[1;32m--> 497\u001b[1;33m                 \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    498\u001b[0m             )\n\u001b[0;32m    499\u001b[0m         )\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid comparison between dtype=int64 and str"
     ]
    }
   ],
   "source": [
    "# The fuzzy join\n",
    "nq_geodf[['geo1_match_name','geo1_match_score','geo1_match_index']] \\\n",
    "= fuzzy_join(nq_geodf, ipums_geodf, 'geo1_name')[['name','score','index']]\n",
    "nq_geodf[['geo2_match_name','geo2_match_score','geo2_match_index']] \\\n",
    "= fuzzy_join(nq_geodf, ipums_geodf, 'geo2_name', \n",
    "             subset=['geo1_match_name','geo1_name']\n",
    "            )[['name','score','index']]\n",
    "\n",
    "# confirmation of results\n",
    "print(\n",
    "    len(nq_geodf[nq_geodf.duplicated('geo2_code', keep=False)].sort_values('geo2_code')\\\n",
    "    [[\"geo1_code\",\"geo2_code\",\n",
    "      \"geo1_name\",\"geo1_match_name\",\n",
    "      \"geo2_name\",\"geo2_match_name\",\n",
    "      \"count\"]]\n",
    "    )==0\n",
    ")      \n",
    "    \n",
    "# determination of cutoff\n",
    "print(\n",
    "      nq_geodf[(nq_geodf.geo2_match_score<80)][['geo1_name',\n",
    "                                          'geo1_code',\n",
    "#                                         'geo1_match_name',\n",
    "                                        'geo2_name',\n",
    "                                        'geo2_code',\n",
    "                                        'geo2_match_name',\n",
    "                                        'geo2_match_score'\n",
    "                                       ]\n",
    "                ].sort_values('geo1_name'))\n",
    "cutoff = input(\"AR: Decide at what level to cut off matches: \")\n",
    "has_ipums_geo = nq_geodf.geo2_match_score>cutoff \n",
    "\n",
    "# attaching geography codes\n",
    "nq_geodf['IPUMS_geo2_code'] = np.nan\n",
    "\n",
    "nq_geodf.loc[has_ipums_geo,'IPUMS_geo2_code'] = nq_geodf[has_ipums_geo]\\\n",
    "                            .geo2_match_index\\\n",
    "                            .astype(int)\\\n",
    "                            .apply(\n",
    "                                lambda i: ipums_geodf.loc[i,'geo2_code']\n",
    "                            .astype(int)\n",
    ")\n",
    "                            \n",
    "# Merge centroids onto NQ geometries:\n",
    "nq_geodf_merged = nq_geodf.merge(geo2_centroids[['ADMIN_NAME','Y','X','IPUM'+year]], \n",
    "               left_on='IPUMS_geo2_code',\n",
    "               right_on=\"IPUM\"+year,\n",
    "               how='left'\n",
    "              ).drop('IPUM'+year, axis=1)\n",
    "#Merge NQ geometries onto NQ data:\n",
    "panel_geo = netquest.merge(nq_geodf_merged[['X','Y','geo2_code']],\n",
    "               left_on=geo2_nq,\n",
    "               right_on='geo2_code',\n",
    "               how='left'\n",
    "              )\n",
    "#Merge census geometries onto census data\n",
    "census_geo = census.merge(geo2_centroids[['ADMIN_NAME','X',\"Y\",'IPUM'+year]],\n",
    "                          left_on = geo2_ipums,\n",
    "                          right_on='IPUM'+year,\n",
    "                          how='left'\n",
    "                         ).drop('IPUM'+year,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['p_codigo', 'p_sexo', 'panelistAge', 'pv_fecha_modificacion', 'pais',\n",
       "       'CodigoPostal', 'AR_provincia', 'AR_localidad', 'BR_estado',\n",
       "       'BR_cidade',\n",
       "       ...\n",
       "       'P8', 'P10', 'P11', 'P12', 'P13', 'CLCSOCIAL2012', 'CLCSOCIAL2015',\n",
       "       'CLCSOCIAL2018', 'MXCSOCIAL2018', 'AR_departamento'],\n",
       "      dtype='object', length=187)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netquest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if panel_geo.shape[0]!=netquest.shape[0]:\n",
    "    print(\"Problem with panel shape match\")\n",
    "if census_geo.shape[0]!=census.shape[0]:\n",
    "    print(\"Problem with census shape match\")\n",
    "panel_geo.to_csv(panelout)\n",
    "census_geo.to_csv(ipumsout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
