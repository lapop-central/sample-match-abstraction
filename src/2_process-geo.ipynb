{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz, process\n",
    "#import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining specific parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = \"PE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "if country == \"AR\":\n",
    "    geo1_nq = \"AR_provincia\"\n",
    "    geo2_nq = \"AR_departamento\"\n",
    "    geo1_ipums = 'GEO1_AR2010'\n",
    "    geo2_ipums = 'GEO2_AR2010'\n",
    "    year = '2010'\n",
    "    ipumsfile = \"../../raw/ipums/\"+country+\"/ipumsi_00015.csv\"\n",
    "    netquestfile = \"C:/Users/schadem/Box Sync/LAPOP Shared/working documents/maita/Coordination/IDB Online Crime/Matching process/Data/AR/panel/AR_netquest-panel.csv\"#\"../out/panel_country/\"+country+\"_netquest-panel.csv\"\n",
    "    dictfile = \"C:/Users/schadem/Box Sync/LAPOP Shared/working documents/maita/Coordination/IDB Online Crime/Matching process/Data/AR/panel/AR_levels.xlsx\"#\"../out/panel_country/\"+country+\"_levels.xlsx\"\n",
    "    geo2file = '../out/ipums_country/ipums_codebook_'+geo2_ipums+'.csv'\n",
    "    geo1file = '../out/ipums_country/ipums_codebook_'+geo1_ipums+'.csv'\n",
    "    centroidfile = '../out/geo/'+country+'_geo2_centroids.csv'\n",
    "\n",
    "elif country == \"BR\":\n",
    "    geo1_nq = \"BR_estado\"\n",
    "    geo2_nq = \"BR_cidade\"\n",
    "    geo1_ipums = 'GEO1_BR2010'\n",
    "    geo2_ipums = 'GEO2_BR2010'\n",
    "    year = '2010'\n",
    "    ipumsfile = \"../../raw/ipums/\"+country+\"/ipumsi_00026.csv\"\n",
    "    \n",
    "elif country == \"CL\":\n",
    "    geo1_nq = \"CL_provincia\"\n",
    "    geo2_nq = \"CL_comuna\"\n",
    "    geo1_ipums = 'GEO1_CL2002'\n",
    "    geo2_ipums = 'GEO2_CL2002'\n",
    "    year = '2002'\n",
    "    ipumsfile = \"../../raw/ipums/\"+country+\"/ipumsi_00020.csv\"\n",
    "    \n",
    "elif country==\"CO\":\n",
    "    geo2_nq = \"CO_municipio\"\n",
    "    geo1_nq = \"CO_departamento\"\n",
    "    geo1_ipums = 'GEO1_CO2005'\n",
    "    geo2_ipums = 'GEO2_CO2005'\n",
    "    year = '2005'\n",
    "    ipumsfile = \"../../raw/ipums/\"+country+\"/ipumsi_00022.csv\"\n",
    "    \n",
    "elif country==\"MX\":\n",
    "    geo1_nq = \"MX_int_estado\"\n",
    "    geo2_nq = \"int_municipio_delegacion\"\n",
    "    geo1_ipums = 'GEO1_MX2015'\n",
    "    geo2_ipums = 'GEO2_MX2015'\n",
    "    year = '2015'\n",
    "    ipumsfile = \"../../raw/ipums/\"+country+\"/ipumsi_00021.csv\"\n",
    "    \n",
    "elif country==\"PE\":\n",
    "    geo2_nq = \"PE_provincia\"\n",
    "    geo1_nq = \"PE_departamento\"\n",
    "    geo1_ipums = \"GEO1_PE2007\" # department\n",
    "    geo2_ipums = \"GEO2_PE2007\" # province\n",
    "    year = '2007'\n",
    "    ipumsfile = \"../../raw/ipums/\"+country+\"/ipumsi_00030.csv\"\n",
    "    \n",
    "netquestfile = \"../out/panel_country/\"+country+\"_netquest-panel.csv\"\n",
    "dictfile = \"../out/panel_country/\"+country+\"_levels.xlsx\"\n",
    "geo2file = '../out/ipums_country/ipums_codebook_'+geo2_ipums+'.csv'\n",
    "geo1file = '../out/ipums_country/ipums_codebook_'+geo1_ipums+'.csv'\n",
    "centroidfile = '../out/geo/'+country+'_geo_centroids.csv'\n",
    "panelout = '../out/panel_country/'+country+'_netquest-panel_geo.csv'\n",
    "ipumsout = '../out/ipums_country/'+country+'_ipums-census_geo.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../out/panel_country/PE_netquest-panel.csv'"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netquestfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "#census = pd.read_csv(ipumsfile)\n",
    "netquest = pd.read_csv(netquestfile)\n",
    "nq_dict = pd.read_excel(dictfile)\n",
    "if country==\"PE\":\n",
    "    ipums_geo2 = pd.read_csv(geo2file, names=['code','name'], skiprows=1)\n",
    "    ipums_geo1 = pd.read_csv(geo1file, names=['code','name'], skiprows=1)\n",
    "else:\n",
    "    ipums_geo2 = pd.read_csv(geo2file,encoding='Latin1', names=['code','name'], skiprows=1)\n",
    "    ipums_geo1 = pd.read_csv(geo1file,encoding='Latin1', names=['code','name'], skiprows=1)\n",
    "\n",
    "\n",
    "geo2_centroids = pd.read_csv(centroidfile, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PECSOCIAL',\n",
       " 'PE_cp',\n",
       " 'PE_education_level',\n",
       " 'PE_education_level_hhousehold',\n",
       " 'PE_laboral_situation',\n",
       " 'PE_laboral_situation_hhousehold',\n",
       " 'PE_NSE_bath',\n",
       " 'PE_NSE_cars',\n",
       " 'PE_NSE_health',\n",
       " 'PE_NSE_maid',\n",
       " 'PE_NSE_pavement',\n",
       " 'PE_NSE_walls',\n",
       " 'PE_NSE_comodities#1',\n",
       " 'PE_NSE_comodities#2',\n",
       " 'PE_NSE_comodities#3',\n",
       " 'PE_NSE_comodities#4',\n",
       " 'PE_NSE_comodities#5',\n",
       " 'PE_NSE_comodities#6',\n",
       " 'PE_NSE_comodities#7',\n",
       " 'PE_NSE_comodities#8',\n",
       " 'PE_NSE_pavement_2017',\n",
       " 'PE_P14',\n",
       " 'PECSOCIAL2017',\n",
       " 'PE_departamento',\n",
       " 'PE_provincia',\n",
       " 'PE_distrito']"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k for k in netquest.columns if k.startswith('PE')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one for fuzzy-joining dataframes\n",
    "def fuzzy_join(df1, df2, varname, subset=False, cutoff=90):\n",
    "    '''Returns a dataframe the length of df1, with the matches on the given variable\n",
    "    \n",
    "        Assumes the var is named the same in both dataframes.\n",
    "        \n",
    "        subset is a tuple or list of length 2 whose first element is a string indicating which\n",
    "        variable in df1 has to be equal to which variable in df2 (specified by 2nd element)\n",
    "        '''\n",
    "    #figure out if there is a constraint\n",
    "    if not subset:\n",
    "        #match #1\n",
    "        ratio_matches = df1.astype(str).apply(\n",
    "                lambda d: process.extract(d[varname], \n",
    "                                             df2[varname].astype(str).drop_duplicates(),\n",
    "                                             scorer=fuzz.ratio, limit=2\n",
    "                                            ), axis=1)\n",
    "        #match #2\n",
    "        parti_matches = df1.astype(str).apply(\n",
    "                lambda d: process.extract(d[varname], \n",
    "                                             df2[varname].astype(str).drop_duplicates(),\n",
    "                                             scorer=fuzz.partial_ratio, limit=2\n",
    "                                            ), axis=1)\n",
    "        \n",
    "    else:\n",
    "        #match #1\n",
    "        ratio_matches = df1.astype(str).apply(\n",
    "                lambda d: process.extract(d[varname], \n",
    "                                             df2[varname][df2[subset[1]]==d[subset[0]]].astype(str).drop_duplicates(),\n",
    "                                             scorer=fuzz.ratio, limit=2\n",
    "                                            ), axis=1)\n",
    "        #match #2\n",
    "        parti_matches = df1.astype(str).apply(\n",
    "                lambda d: process.extract(d[varname], \n",
    "                                             df2[varname][df2[subset[1]]==d[subset[0]]].astype(str).drop_duplicates(),\n",
    "                                             scorer=fuzz.partial_ratio, limit=2\n",
    "                                            ), axis=1)    \n",
    "\n",
    "    \n",
    "    # different match cases\n",
    "    #morRatioMatch = ratio_matches.apply(lambda l: (l[0][1]==100)&(l[1][1]==100))\n",
    "    oneRatioMatch = ratio_matches.apply(lambda l: (l[0][1]>=cutoff))#&(l[1][1]<100))\n",
    "    noRatioMatch = ratio_matches.apply(lambda l: (l[0][1]<cutoff))\n",
    "\n",
    "    #morPartiMatch = parti_matches.apply(lambda l: (l[0][1]==100)&(l[1][1]==100))\n",
    "    onePartiMatch = parti_matches.apply(lambda l: (l[0][1]>=cutoff))#&(l[1][1]<100))\n",
    "    noPartiMatch = parti_matches.apply(lambda l: (l[0][1]<cutoff))\n",
    "    \n",
    "    # pick out what's better\n",
    "    matches = pd.Series([(np.nan,np.nan,np.nan)]*len(df1),index=df1.index)\n",
    "    matches.loc[oneRatioMatch] = ratio_matches.loc[oneRatioMatch].apply(lambda l: l[0])\n",
    "    matches.loc[(noRatioMatch&onePartiMatch)] = parti_matches.loc[(noRatioMatch&onePartiMatch)]\\\n",
    "        .apply(lambda l: l[0])\n",
    "    matches.loc[(noRatioMatch&noPartiMatch)] = parti_matches.loc[(noRatioMatch&noPartiMatch)]\\\n",
    "        .apply(lambda l: l[0])\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df['name'] = matches.apply(lambda l: l[0])\n",
    "    df['score'] = matches.apply(lambda l: l[1])\n",
    "    df['index'] = matches.apply(lambda l: l[2])\n",
    "    df['parti_matches'] = parti_matches\n",
    "    df['ratio_matches'] = ratio_matches\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up helper structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\schadem\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\schadem\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\schadem\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "nq_geo1 = nq_dict[nq_dict.Variable==geo1_nq]\n",
    "nq_geo1.columns = [\"Variable\",geo1_nq,geo1_nq+\"_name\"]\n",
    "nq_geo1 = nq_geo1[[geo1_nq,geo1_nq+\"_name\"]]\n",
    "\n",
    "nq_geo2 = nq_dict[nq_dict.Variable==geo2_nq]\n",
    "nq_geo2.columns = [\"Variable\",geo2_nq,geo2_nq+\"_name\"]\n",
    "nq_geo2 = nq_geo2[[geo2_nq,geo2_nq+\"_name\"]]\n",
    "\n",
    "# setting up unique DF for geographies\n",
    "ipums_geodf = census[[geo1_ipums,geo2_ipums]]\n",
    "ipums_geodf.columns = ['geo1_code','geo2_code']\n",
    "ipums_geodf.drop_duplicates(subset=['geo1_code','geo2_code'],inplace=True)\n",
    "\n",
    "ipums_geodf['geo1_name'] = ipums_geodf.merge(ipums_geo1, \n",
    "                                             how='left', \n",
    "                                             left_on = \"geo1_code\", \n",
    "                                             right_on = 'code', \n",
    "                                             copy=False)['name'].values\n",
    "\n",
    "ipums_geodf['geo2_name'] = ipums_geodf.merge(ipums_geo2, \n",
    "                                             how='left', \n",
    "                                             left_on = \"geo2_code\", \n",
    "                                             right_on = 'code', \n",
    "                                             copy=False)['name'].values\n",
    "\n",
    "# Melting doubled-up geographies\n",
    "ipums_geodf = ipums_geodf.geo2_name.str.split(',').apply(pd.Series) \\\n",
    "    .merge(ipums_geodf, right_index = True, left_index = True) \\\n",
    "    .drop([\"geo2_name\"], axis = 1) \\\n",
    "    .melt(id_vars = [k for k in ipums_geodf.columns if not (type(k)==int)|(k=='geo2_name')], value_name = \"geo2_name\") \\\n",
    "    .drop(\"variable\", axis = 1) \\\n",
    "    .dropna(subset=['geo2_name'])\n",
    "ipums_geodf.geo2_name = ipums_geodf.geo2_name.str.strip()\n",
    "\n",
    "ipums_geodf = ipums_geodf.geo1_name.str.split(',').apply(pd.Series) \\\n",
    "    .merge(ipums_geodf, right_index = True, left_index = True) \\\n",
    "    .drop([\"geo1_name\"], axis = 1) \\\n",
    "    .melt(id_vars = [k for k in ipums_geodf.columns if not (type(k)==int)|(k=='geo1_name')], value_name = \"geo1_name\") \\\n",
    "    .drop(\"variable\", axis = 1) \\\n",
    "    .dropna(subset=['geo1_name'])\n",
    "ipums_geodf.geo1_name = ipums_geodf.geo1_name.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Country-specific pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "if country==\"AR\":\n",
    "    # accounting for the CABA mess\n",
    "    ipums_geodf.loc[ipums_geodf.geo1_name==\"City of Buenos Aires\",\n",
    "             'geo1_name'] = 'CABA'\n",
    "    ipums_geodf.loc[(ipums_geodf.geo1_name==\"CABA\"),'geo2_name'] = 'CABA'\n",
    "    caba_codes = ipums_geodf[ipums_geodf.geo1_name==\"CABA\"].geo2_code\n",
    "    # dropping CABA-related duplicates\n",
    "    ipums_geodf.drop_duplicates(subset=[\"geo1_name\",\"geo2_name\"], inplace=True)\n",
    "    # make sure there is only one CABA-code left\n",
    "    #print(len(ipums_geodf[ipums_geodf.geo1_name==\"CABA\"].geo2_code.values[0])==1)\n",
    "    census.loc[census[geo2_ipums].isin(caba_codes),geo2_ipums] = \\\n",
    "    len(census.loc[census[geo2_ipums].isin(caba_codes),geo2_ipums])*[ipums_geodf[ipums_geodf.geo1_name==\"CABA\"].geo2_code[0]]\n",
    "    \n",
    "    # give CABA locations without geo2 a specific code: 999\n",
    "    netquest.loc[netquest[geo2_nq].isna()&(netquest[geo1_nq]==1),geo2_nq] = 999\n",
    "    nq_geodf = netquest.merge(nq_geo2, on=geo2_nq, how='left')\\\n",
    "                       .merge(nq_geo1, on=geo1_nq, how='left')[[geo1_nq,geo1_nq+'_name',geo2_nq,geo2_nq+'_name']]\n",
    "    nq_geodf.columns = ['geo1_code','geo1_name','geo2_code','geo2_name']\n",
    "    nq_geodf.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Fixing CABA not having Dept.\n",
    "    # Where province is CABA and no departamento exists, call these \"CABA\"\n",
    "    nq_geodf.loc[nq_geodf.geo1_name==\"Ciudad Autónoma de Buenos Aires\",\n",
    "                 'geo1_name'] = 'CABA'\n",
    "    nq_geodf = nq_geodf[(nq_geodf['geo1_code'].notna()) & (nq_geodf['geo2_code'].notna())] \n",
    "\n",
    "else:\n",
    "    nq_geodf = netquest.merge(nq_geo2, on=geo2_nq, how='left')\\\n",
    "                       .merge(nq_geo1, on=geo1_nq, how='left')[[geo1_nq,geo1_nq+'_name',geo2_nq,geo2_nq+'_name']]\n",
    "    nq_geodf.columns = ['geo1_code','geo1_name','geo2_code','geo2_name']\n",
    "\n",
    "    nq_geodf.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Country-independent pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the duplicates in nq_geodf.geo2_code, and keeping the combinations that occur most often (which are quite clearly the good ones).\n",
    "nq_geodf['count'] = nq_geodf.apply(\n",
    "    lambda r: sum((netquest[geo1_nq]==r['geo1_code'])&\n",
    "                  (netquest[geo2_nq]==r['geo2_code'])),\n",
    "                                   axis=1)\n",
    "nq_geodf = nq_geodf.sort_values(['geo2_code','count']) \\\n",
    "    [~(nq_geodf.sort_values(['geo2_code','count']).duplicated('geo2_code',keep='last'))]\n",
    "\n",
    "nq_geodf.geo1_name = nq_geodf.geo1_name.str.title()\n",
    "nq_geodf.geo2_name = nq_geodf.geo2_name.str.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geo1_code</th>\n",
       "      <th>geo1_name</th>\n",
       "      <th>geo2_code</th>\n",
       "      <th>geo2_name</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3076</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Amazonas</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Chachapoyas</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5468</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Amazonas</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Bagua</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80696</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Amazonas</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Bongara</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15504</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Amazonas</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Condorcanqui</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55014</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Amazonas</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Luya</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242478</th>\n",
       "      <td>25.0</td>\n",
       "      <td>Ucayali</td>\n",
       "      <td>195.0</td>\n",
       "      <td>Purus</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>Lima</td>\n",
       "      <td>196.0</td>\n",
       "      <td>Lima</td>\n",
       "      <td>61802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53296</th>\n",
       "      <td>16.0</td>\n",
       "      <td>Loreto</td>\n",
       "      <td>197.0</td>\n",
       "      <td>Datem Del Marañon</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198378</th>\n",
       "      <td>16.0</td>\n",
       "      <td>Loreto</td>\n",
       "      <td>198.0</td>\n",
       "      <td>Putumayo</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336604</th>\n",
       "      <td>15.0</td>\n",
       "      <td>Lima</td>\n",
       "      <td>1856.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>194 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        geo1_code geo1_name  geo2_code          geo2_name  count\n",
       "3076          1.0  Amazonas        1.0        Chachapoyas    184\n",
       "5468          1.0  Amazonas        2.0              Bagua     96\n",
       "80696         1.0  Amazonas        3.0            Bongara     15\n",
       "15504         1.0  Amazonas        4.0       Condorcanqui      7\n",
       "55014         1.0  Amazonas        5.0               Luya     14\n",
       "...           ...       ...        ...                ...    ...\n",
       "242478       25.0   Ucayali      195.0              Purus      1\n",
       "2            15.0      Lima      196.0               Lima  61802\n",
       "53296        16.0    Loreto      197.0  Datem Del Marañon      6\n",
       "198378       16.0    Loreto      198.0           Putumayo      2\n",
       "336604       15.0      Lima     1856.0                NaN      1\n",
       "\n",
       "[194 rows x 5 columns]"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nq_geodf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More country-specific processing--manual fixes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "if country==\"AR\":\n",
    "    nq_geodf.loc[nq_geodf.geo2_code==999, 'geo2_name'] = \"CABA\"\n",
    "\n",
    "    ipums_geodf.loc[ipums_geodf.geo1_name.str.contains(\"Buenos Aires province\"),\n",
    "                 'geo1_name'] = \"Buenos Aires\"\n",
    "    \n",
    "    ipums_geodf.loc[ipums_geodf.geo2_name.str.contains(\"Puan\"),\n",
    "                    'geo2_name'] = \"Puán\"\n",
    "    ipums_geodf.loc[(ipums_geodf.geo2_name.str.contains(\"General San Martín\"))\\\n",
    "                    &(ipums_geodf.geo1_name==\"Buenos Aires\"),\n",
    "                    'geo2_name'] = \"Ciudad Libertador San Martín\"\n",
    "    ipums_geodf.loc[(ipums_geodf.geo2_name.str.contains(\"La Capital\"))\\\n",
    "                    &(ipums_geodf.geo1_name==\"San Luis\"),\n",
    "                    'geo2_name'] = \"Juan Martín de Pueyrredón\"\n",
    "    ipums_geodf.loc[(ipums_geodf.geo2_name==\"Maipú\")\\\n",
    "                    &(ipums_geodf.geo2_code==6050),\n",
    "                    'geo2_name'] = \"Marcos Paz\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    ipums_geodf.loc[(ipums_geodf.geo2_name.str.contains(\"Chascomus\")),\n",
    "                    'geo2_name'] = \"Chascomús\"\n",
    "    ipums_geodf.loc[(ipums_geodf.geo2_name.str.contains(\"Jose C. Paz\")),\n",
    "                    'geo2_name'] = \"José C. Paz\"\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name.str.contains(\"Paso de Indios\"))\\\n",
    "                    &(nq_geodf.geo1_name==\"Chubut\"),\n",
    "                    'geo2_name'] = \"Paso de los Indios\"\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name.str.contains(\"Coronel de Marina Leonardo Rosales\"))\\\n",
    "                    &(nq_geodf.geo1_name==\"Buenos Aires\"),\n",
    "                    'geo2_name'] = \"Coronel de Marine L. Rosales\"\n",
    "    ipums_geodf.loc[(ipums_geodf.geo2_name.str.contains(\"Veinticinco de Mayo\"))\\\n",
    "                    &(ipums_geodf.geo1_name==\"Buenos Aires\"),\n",
    "                    'geo2_name'] = \"25 de Mayo\"\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name.str.contains(\"Pueyrredón\"))\\\n",
    "                    &(nq_geodf.geo1_name==\"Buenos Aires\"),\n",
    "                    'geo2_name'] = \"General Pueyrredón\"\n",
    "    \n",
    "elif country==\"BR\":\n",
    "    ipums_geodf.loc[ipums_geodf.geo1_name.str.contains(\"Federal District\"),\n",
    "                'geo1_name'] = \"Distrito Federal\"\n",
    "\n",
    "\n",
    "    ipums_geodf.loc[(ipums_geodf.geo2_name.str.contains(\"Santarém\"))& \\\n",
    "                    (ipums_geodf.geo1_name.str.contains(\"Paraíba\")),\n",
    "                    'geo2_name'] = \"Joca Claudino\"\n",
    "\n",
    "    ipums_geodf.loc[(ipums_geodf.geo2_name.str.contains(\"Presidente Juscelino\"))& \\\n",
    "                    (ipums_geodf.geo1_name.str.contains(\"Rio Grande do Norte\")),\n",
    "                    'geo2_name'] = \"Serra Caiada\"\n",
    "\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name.str.contains(\"Bonfim\"))& \\\n",
    "                    (nq_geodf.geo1_name.str.contains(\"Rio de Janeiro\")),\n",
    "                    'geo1_name'] = \"Minas Gerais\"\n",
    "\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name.str.contains(\"Iracema\"))& \\\n",
    "                    (nq_geodf.geo1_name.str.contains(\"Santa Catarina\")),\n",
    "                    'geo2_name'] = \"Itaiópolis\"\n",
    "\n",
    "\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Pescaria Brava\",'geo2_name'] = 'Laguna'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Balneário Rincão\",'geo2_name'] = 'Içara'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Pinto Bandeira\",'geo2_name'] = 'Bento Gonçalves'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Paraíso das Águas\",'geo2_name'] = 'Água Clara'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Açailândia\",'geo1_name'] = 'Maranhão'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Mariana\",'geo1_name'] = 'Minas Gerais'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Fortaleza\",'geo1_name'] = 'Ceará'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name.isin([\"Leme\",\n",
    "                                          \"São Paulo\",\n",
    "                                          'Itapevi',\n",
    "                                          \"Franca\",\n",
    "                                          \"Presidente Prudente\",\n",
    "                                          \"Suzano\",\n",
    "                                          \"Ourinhos\"\n",
    "                                         ]),'geo1_name'] = 'São Paulo'\n",
    "\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name==\"Praia Grande\") &\\\n",
    "                 ((nq_geodf.geo1_name==\"Acre\")|(nq_geodf.geo1_name==\"Maranhão\")),\n",
    "                 'geo1_name'] = 'São Paulo'\n",
    "\n",
    "    nq_geodf.loc[nq_geodf.geo1_name==\"Brasília\",'geo1_name'] = 'Distrito Federal'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Brasília\",'geo1_name'] = 'Distrito Federal'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Tupanatinga\",'geo1_name'] = 'Pernambuco'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Macaé\",'geo1_name'] = 'Rio de Janeiro'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Queimados\",'geo1_name'] = 'Rio de Janeiro'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"São Miguel do Iguaçu\",'geo1_name'] = 'Paraná'\n",
    "    \n",
    "elif country==\"CL\":\n",
    "    nq_geodf.geo1_name = nq_geodf.geo1_name.str.replace(\"Provincia \",\"\")\n",
    "    nq_geodf.geo1_name = nq_geodf.geo1_name.str.replace(\"de \",\"\")\n",
    "    nq_geodf.loc[nq_geodf.geo1_name==\"Marga Marga\",'geo1_name'] = 'Valparaíso' #(?)\n",
    "    nq_geodf.loc[nq_geodf.geo1_name==\"Tamarugal\",'geo1_name'] = 'Valparaíso'\n",
    "    nq_geodf.loc[nq_geodf.geo1_name==\"Aysen\",'geo1_name'] = 'Aisén'\n",
    "    nq_geodf.loc[nq_geodf.geo1_name==\"Diguillin\",'geo1_name'] = 'Ñuble'\n",
    "    nq_geodf.loc[nq_geodf.geo1_name==\"Itata\",'geo1_name'] = 'Ñuble'\n",
    "    nq_geodf.loc[nq_geodf.geo1_name==\"Punilla\",'geo1_name'] = 'Ñuble'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Alto Hospicio\",'geo2_name'] = 'Iquique'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Aysen\",'geo2_name'] = 'Aisén'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Hualpén\",'geo2_name'] = 'Talcahuano'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Cholchol\",'geo2_name'] = 'Nueva Imperial'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Alto Bío-Bío\",'geo2_name'] = 'Santa Bárbara'\n",
    "    \n",
    "    ipums_geodf.loc[ipums_geodf.geo2_name.isin([\"Pozo Almonte\",\"Colchane\",'Pica','Huara',\"Camiña\"]),\n",
    "                'geo1_name'] = \"Valparaíso\"\n",
    "    ipums_geodf.loc[ipums_geodf.geo2_name.isin([\"San Fernando\",\"Palmilla\", \"Chimbarongo\", \n",
    "                                            \"Nancagua\",\"Chépica\",\"Pumanque\",\"Placilla\",\n",
    "                                            \"Lolol\", \"Peralillo\", \"Santa Cruz\"\n",
    "                                           ]),\n",
    "               'geo1_name'] = \"Colchagua\"\n",
    "    ipums_geodf.loc[ipums_geodf.geo2_name.isin([\"Lago Ranco\", \"Futrono\", \"Río Bueno\", \n",
    "                                                \"La Unión\"\n",
    "                                           ]),\n",
    "               'geo1_name'] = \"Ranco\"\n",
    "    ipums_geodf.loc[ipums_geodf.geo2_name.str.contains(\"Olmué\"),'geo1_name'] = 'Valparaíso'\n",
    "    ipums_geodf.loc[ipums_geodf.geo2_name==\"Limache\",'geo1_name'] = 'Valparaíso'\n",
    "    \n",
    "elif country==\"CO\":\n",
    "    ipums_geodf.loc[(ipums_geodf.geo1_name==\"Guania\"),'geo1_name'] = 'Guainía'\n",
    "    ipums_geodf.loc[(ipums_geodf.geo1_name==\"Valle\"),'geo1_name'] = 'Valle del Cauca'\n",
    "    ipums_geodf.loc[(ipums_geodf.geo2_name==\"Itagui\"),'geo2_name'] = 'Itagüí'\n",
    "    \n",
    "    nq_geodf.loc[(nq_geodf.geo2_name==\"Ubaté\"),'geo2_name'] = 'Villa de San Diego de Ubate'\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name==\"Machetá\"),'geo2_name'] = 'Macheta'\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name==\"Tuchin\"),'geo2_name'] = 'San Andrés Sotavento'\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name==\"Santa Cruz de Mompox\"),'geo2_name'] = 'Mompós'\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name==\"Ramiquirí\"),'geo2_name'] = 'Ramiriquí'\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name==\"Bogotá Distrito Capital (D. C.)\"),'geo2_name'] = 'Bogotá'\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name==\"Anzá\"),'geo2_name'] = 'Anza'\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name==\"Suán\"),'geo2_name'] = 'Suan'\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name==\"Villa Gamuez (La Hormiga)\"),'geo2_name'] = 'Valle del Guamuez'\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name==\"Imúes\"),'geo2_name'] = 'Imués'\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name==\"San Jose de Ure\"),'geo2_name'] = 'Montelíbano'\n",
    "    nq_geodf.loc[((nq_geodf.geo2_name==\"Guachené\") | (nq_geodf.geo2_name==\"Guachene\")\n",
    "                 ),'geo2_name'] = 'Caloto'\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name==\"Capitanejo\"),'geo1_name'] = 'Santander'\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name==\"Bojacá\"),'geo1_name'] = 'Cundinamarca'\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name==\"Beltrán\"),'geo1_name'] = 'Cundinamarca'\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name==\"Soacha\"),'geo1_name'] = 'Cundinamarca'\n",
    "    \n",
    "elif country==\"MX\":\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name==\"Mexicali\")&(~nq_geodf.geo2_name.isna()),'geo1_name'] = 'Baja California'\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name==\"Xochimilco\")&(~nq_geodf.geo2_name.isna()),'geo1_name'] = 'Ciudad de México'\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name==\"Coacalco de Berriozábal\")&(~nq_geodf.geo2_name.isna()),'geo1_name'] = 'México'\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name.str.contains(\"Murguía\"))&(~nq_geodf.geo2_name.isna()),'geo1_name'] = 'Zacatecas'\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name==\"Chimalhuacán\")&(~nq_geodf.geo2_name.isna()),'geo1_name'] = 'México'\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name==\"Ecatepec de Morelos\")&(~nq_geodf.geo2_name.isna()),'geo1_name'] = 'México'\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name==\"Querétaro\")&(~nq_geodf.geo2_name.isna()),'geo1_name'] = 'Querétaro'\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name==\"Tepic\")&(~nq_geodf.geo2_name.isna()),'geo1_name'] = 'Nayarit'\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name==\"Coyoacán\")&(~nq_geodf.geo2_name.isna()),'geo1_name'] = 'Ciudad de México'\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name==\"Mérida\")&(~nq_geodf.geo2_name.isna()),'geo1_name'] = 'Yucatán'\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name==\"Puerto Morelos\")&(~nq_geodf.geo2_name.isna()),'geo2_name'] = 'Benito Juárez'\n",
    "    nq_geodf.loc[(nq_geodf.geo1_name.str.contains(\"Baja California S\"))&(~nq_geodf.geo1_name.isna()),\n",
    "                    'geo1_name'] = \"Baja California Sur\"\n",
    "    ipums_geodf.loc[ipums_geodf.geo1_name.str.contains(\"Distrito Federal\"),\n",
    "                    'geo1_name'] = \"Ciudad de México\"\n",
    "    nq_geodf.loc[nq_geodf.geo2_name.str.contains(\"Túxpam\")&(~nq_geodf.geo2_name.isna()),\n",
    "                    'geo2_name'] = \"Tuxpan\"\n",
    "\n",
    "    nq_geodf.geo2_name = nq_geodf.geo2_name.str.replace(\"- Dto.\", \"Distrito\")\\\n",
    "                                .str.replace(\"Dr.\", \"Doctor\")\\\n",
    "                                .str.replace(\"Gral.\", \"General\")\n",
    "    \n",
    "elif country==\"PE\":\n",
    "    ipums_geodf.loc[ipums_geodf.geo2_name==(\"Huzánuco\"),\n",
    "                    'geo2_name'] = \"Huánuco\" \n",
    "    nq_geodf.loc[nq_geodf.geo2_name==(\"Putumayo\"),\n",
    "                    'geo2_name'] = \"Maynas\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are there no more duplicates?\n",
      "True\n",
      "Showing problematic geographies...\n",
      "        geo1_name  geo1_code geo1_match_name                  geo2_name  \\\n",
      "100474     Ancash        2.0          Ancash  Carlos Fermin Fitzcarrald   \n",
      "312     Cajamarca        6.0       Cajamarca                       Jaen   \n",
      "336604       Lima       15.0            Lima                        NaN   \n",
      "\n",
      "        geo2_code        geo2_match_name  geo2_match_score  \n",
      "100474       14.0  Carlos F. Fitzcarrald                76  \n",
      "312          61.0                   Jaén                75  \n",
      "336604     1856.0               Barranca                67  \n",
      "PE: Decide at what level to cut off matches: 70\n"
     ]
    }
   ],
   "source": [
    "# The fuzzy join\n",
    "nq_geodf[['geo1_match_name','geo1_match_score','geo1_match_index']] \\\n",
    "= fuzzy_join(nq_geodf, ipums_geodf, 'geo1_name')[['name','score','index']]\n",
    "nq_geodf[['geo2_match_name','geo2_match_score','geo2_match_index']] \\\n",
    "= fuzzy_join(nq_geodf, ipums_geodf, 'geo2_name', \n",
    "             subset=['geo1_match_name','geo1_name']\n",
    "            )[['name','score','index']]\n",
    "\n",
    "# confirmation of results\n",
    "print(\"Are there no more duplicates?\")\n",
    "print(\n",
    "    len(nq_geodf[nq_geodf.duplicated('geo2_code', keep=False)].sort_values('geo2_code')\\\n",
    "    [[\"geo1_code\",\"geo2_code\",\n",
    "      \"geo1_name\",\"geo1_match_name\",\n",
    "      \"geo2_name\",\"geo2_match_name\",\n",
    "      \"count\"]]\n",
    "    )==0\n",
    ")      \n",
    "    \n",
    "# determination of cutoff\n",
    "print(\"Showing problematic geographies...\")\n",
    "print(\n",
    "      nq_geodf[(nq_geodf.geo2_match_score<80)][['geo1_name',\n",
    "                                          'geo1_code',\n",
    "                                        'geo1_match_name',\n",
    "                                        'geo2_name',\n",
    "                                        'geo2_code',\n",
    "                                        'geo2_match_name',\n",
    "                                        'geo2_match_score'\n",
    "                                       ]\n",
    "                ].sort_values('geo1_name'))\n",
    "cutoff = input(country + \": Decide at what level to cut off matches: \")\n",
    "\n",
    "has_ipums_geo = nq_geodf.geo2_match_score>int(cutoff )\n",
    "\n",
    "# attaching geography codes\n",
    "nq_geodf['IPUMS_geo2_code'] = np.nan\n",
    "\n",
    "nq_geodf.loc[has_ipums_geo,'IPUMS_geo2_code'] = nq_geodf[has_ipums_geo]\\\n",
    "                            .geo2_match_index\\\n",
    "                            .astype(int)\\\n",
    "                            .apply(\n",
    "                                lambda i: ipums_geodf.loc[i,'geo2_code']\n",
    "                            .astype(int)\n",
    ")\n",
    "                            \n",
    "# Merge centroids onto NQ geometries:\n",
    "nq_geodf_merged = nq_geodf.merge(geo2_centroids[['ADMIN_NAME','Y','X','IPUM'+year]], \n",
    "               left_on='IPUMS_geo2_code',\n",
    "               right_on=\"IPUM\"+year,\n",
    "               how='left'\n",
    "              ).drop('IPUM'+year, axis=1)\n",
    "#Merge NQ geometries onto NQ data:\n",
    "panel_geo = netquest.merge(nq_geodf_merged[['X','Y','geo2_code']],\n",
    "               left_on=geo2_nq,\n",
    "               right_on='geo2_code',\n",
    "               how='left'\n",
    "              )\n",
    "#Merge census geometries onto census data\n",
    "census_geo = census.merge(geo2_centroids[['ADMIN_NAME','X',\"Y\",'IPUM'+year]],\n",
    "                          left_on = geo2_ipums,\n",
    "                          right_on='IPUM'+year,\n",
    "                          how='left'\n",
    "                         ).drop('IPUM'+year,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "if panel_geo.shape[0]!=netquest.shape[0]:\n",
    "    print(\"Problem with panel shape match\")\n",
    "if census_geo.shape[0]!=census.shape[0]:\n",
    "    print(\"Problem with census shape match\")\n",
    "panel_geo.loc[:,[not(\"Unnamed\" in k) for k in panel_geo.columns]].to_csv(panelout)\n",
    "census_geo.loc[:,[not(\"Unnamed\" in k) for k in census_geo.columns]].to_csv(ipumsout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
