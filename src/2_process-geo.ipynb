{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz, process\n",
    "#import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining specific parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = \"CL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if country == \"AR\":\n",
    "    nq_date = \"190628\"\n",
    "    geo1_nq = \"AR_provincia\"\n",
    "    geo2_nq = \"AR_departamento\"\n",
    "    geo1_ipums = 'GEO1_AR2010'\n",
    "    geo2_ipums = 'GEO2_AR2010'\n",
    "    year = '2010'\n",
    "    ipumsfile = \"../../raw/ipums/\"+country+\"/ipumsi_00015.csv\"\n",
    "    netquestfile = \"C:/Users/schadem/Box Sync/LAPOP Shared/working documents/maita/Coordination/IDB Online Crime/Matching process/Data/AR/panel/AR_netquest-panel.csv\"#\"../out/panel_country/\"+country+\"_netquest-panel.csv\"\n",
    "    dictfile = \"C:/Users/schadem/Box Sync/LAPOP Shared/working documents/maita/Coordination/IDB Online Crime/Matching process/Data/AR/panel/AR_levels.xlsx\"#\"../out/panel_country/\"+country+\"_levels.xlsx\"\n",
    "    geo2file = '../out/ipums_country/ipums_codebook_'+geo2_ipums+'.csv'\n",
    "    geo1file = '../out/ipums_country/ipums_codebook_'+geo1_ipums+'.csv'\n",
    "    centroidfile = '../out/geo/'+country+'_geo2_centroids.csv'\n",
    "\n",
    "elif country == \"BR\":\n",
    "    nq_date = \"190628\"\n",
    "    geo1_nq = \"BR_estado\"\n",
    "    geo2_nq = \"BR_cidade\"\n",
    "    geo1_ipums = 'GEO1_BR2010'\n",
    "    geo2_ipums = 'GEO2_BR2010'\n",
    "    year = '2010'\n",
    "    ipumsfile = \"../../raw/ipums/\"+country+\"/ipumsi_00026.csv\"\n",
    "    netquestfile = \"../out/panel_country/\"+country+\"_netquest-panel.csv\"\n",
    "    dictfile = \"../out/panel_country/\"+country+\"_levels.xlsx\"\n",
    "    geo2file = '../out/ipums_country/ipums_codebook_'+geo2_ipums+'.csv'\n",
    "    geo1file = '../out/ipums_country/ipums_codebook_'+geo1_ipums+'.csv'\n",
    "    centroidfile = '../out/geo/'+country+'_geo_centroids.csv'\n",
    "    \n",
    "elif country == \"CL\":\n",
    "    nq_date = \"191120\"\n",
    "    geo1_nq = \"CL_provincia\"\n",
    "    geo2_nq = \"CL_comuna\"\n",
    "    geo1_ipums = 'GEO1_CL2002'\n",
    "    geo2_ipums = 'GEO2_CL2002'\n",
    "    year = '2002'\n",
    "    ipumsfile = \"../../raw/ipums/\"+country+\"/ipumsi_00020.csv\"\n",
    "    netquestfile = \"../out/panel_country/\"+country+\"_netquest-panel.csv\"\n",
    "    dictfile = \"../out/panel_country/\"+country+\"_levels.xlsx\"\n",
    "    geo2file = '../out/ipums_country/ipums_codebook_'+geo2_ipums+'.csv'\n",
    "    geo1file = '../out/ipums_country/ipums_codebook_'+geo1_ipums+'.csv'\n",
    "    centroidfile = '../out/geo/'+country+'_geo_centroids.csv'\n",
    "    \n",
    "panelout = '../out/panel_country/'+country+'_netquest-panel_geo.csv'\n",
    "ipumsout = '../out/ipums_country/'+country+'_ipums-census_geo.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\schadem\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (22,536,537) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "census = pd.read_csv(ipumsfile)\n",
    "netquest = pd.read_csv(netquestfile)\n",
    "nq_dict = pd.read_excel(dictfile)\n",
    "ipums_geo2 = pd.read_csv(geo2file,encoding='Latin1', names=['code','name'], skiprows=1)\n",
    "ipums_geo1 = pd.read_csv(geo1file,encoding='Latin1', names=['code','name'], skiprows=1)\n",
    "geo2_centroids = pd.read_csv(centroidfile, encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one for fuzzy-joining dataframes\n",
    "def fuzzy_join(df1, df2, varname, subset=False, cutoff=90):\n",
    "    '''Returns a dataframe the length of df1, with the matches on the given variable\n",
    "    \n",
    "        Assumes the var is named the same in both dataframes.\n",
    "        \n",
    "        subset is a tuple or list of length 2 whose first element is a string indicating which\n",
    "        variable in df1 has to be equal to which variable in df2 (specified by 2nd element)\n",
    "        '''\n",
    "    #figure out if there is a constraint\n",
    "    if not subset:\n",
    "        #match #1\n",
    "        ratio_matches = df1.astype(str).apply(\n",
    "                lambda d: process.extract(d[varname], \n",
    "                                             df2[varname].astype(str).drop_duplicates(),\n",
    "                                             scorer=fuzz.ratio, limit=2\n",
    "                                            ), axis=1)\n",
    "        #match #2\n",
    "        parti_matches = df1.astype(str).apply(\n",
    "                lambda d: process.extract(d[varname], \n",
    "                                             df2[varname].astype(str).drop_duplicates(),\n",
    "                                             scorer=fuzz.partial_ratio, limit=2\n",
    "                                            ), axis=1)\n",
    "        \n",
    "    else:\n",
    "        #match #1\n",
    "        ratio_matches = df1.astype(str).apply(\n",
    "                lambda d: process.extract(d[varname], \n",
    "                                             df2[varname][df2[subset[1]]==d[subset[0]]].astype(str).drop_duplicates(),\n",
    "                                             scorer=fuzz.ratio, limit=2\n",
    "                                            ), axis=1)\n",
    "        #match #2\n",
    "        parti_matches = df1.astype(str).apply(\n",
    "                lambda d: process.extract(d[varname], \n",
    "                                             df2[varname][df2[subset[1]]==d[subset[0]]].astype(str).drop_duplicates(),\n",
    "                                             scorer=fuzz.partial_ratio, limit=2\n",
    "                                            ), axis=1)    \n",
    "\n",
    "    \n",
    "    # different match cases\n",
    "    #morRatioMatch = ratio_matches.apply(lambda l: (l[0][1]==100)&(l[1][1]==100))\n",
    "    oneRatioMatch = ratio_matches.apply(lambda l: (l[0][1]>=cutoff))#&(l[1][1]<100))\n",
    "    noRatioMatch = ratio_matches.apply(lambda l: (l[0][1]<cutoff))\n",
    "\n",
    "    #morPartiMatch = parti_matches.apply(lambda l: (l[0][1]==100)&(l[1][1]==100))\n",
    "    onePartiMatch = parti_matches.apply(lambda l: (l[0][1]>=cutoff))#&(l[1][1]<100))\n",
    "    noPartiMatch = parti_matches.apply(lambda l: (l[0][1]<cutoff))\n",
    "    \n",
    "    # pick out what's better\n",
    "    matches = pd.Series([(np.nan,np.nan,np.nan)]*len(df1),index=df1.index)\n",
    "    matches.loc[oneRatioMatch] = ratio_matches.loc[oneRatioMatch].apply(lambda l: l[0])\n",
    "    matches.loc[(noRatioMatch&onePartiMatch)] = parti_matches.loc[(noRatioMatch&onePartiMatch)]\\\n",
    "        .apply(lambda l: l[0])\n",
    "    matches.loc[(noRatioMatch&noPartiMatch)] = parti_matches.loc[(noRatioMatch&noPartiMatch)]\\\n",
    "        .apply(lambda l: l[0])\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df['name'] = matches.apply(lambda l: l[0])\n",
    "    df['score'] = matches.apply(lambda l: l[1])\n",
    "    df['index'] = matches.apply(lambda l: l[2])\n",
    "    df['parti_matches'] = parti_matches\n",
    "    df['ratio_matches'] = ratio_matches\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up helper structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\schadem\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\schadem\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\schadem\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "nq_geo1 = nq_dict[nq_dict.Variable==geo1_nq]\n",
    "nq_geo1.columns = [\"Variable\",geo1_nq,geo1_nq+\"_name\"]\n",
    "nq_geo1 = nq_geo1[[geo1_nq,geo1_nq+\"_name\"]]\n",
    "\n",
    "nq_geo2 = nq_dict[nq_dict.Variable==geo2_nq]\n",
    "nq_geo2.columns = [\"Variable\",geo2_nq,geo2_nq+\"_name\"]\n",
    "nq_geo2 = nq_geo2[[geo2_nq,geo2_nq+\"_name\"]]\n",
    "\n",
    "# setting up unique DF for geographies\n",
    "ipums_geodf = census[[geo1_ipums,geo2_ipums]]\n",
    "ipums_geodf.columns = ['geo1_code','geo2_code']\n",
    "ipums_geodf.drop_duplicates(subset=['geo1_code','geo2_code'],inplace=True)\n",
    "\n",
    "ipums_geodf['geo1_name'] = ipums_geodf.merge(ipums_geo1, \n",
    "                                             how='left', \n",
    "                                             left_on = \"geo1_code\", \n",
    "                                             right_on = 'code', \n",
    "                                             copy=False)['name'].values\n",
    "\n",
    "ipums_geodf['geo2_name'] = ipums_geodf.merge(ipums_geo2, \n",
    "                                             how='left', \n",
    "                                             left_on = \"geo2_code\", \n",
    "                                             right_on = 'code', \n",
    "                                             copy=False)['name'].values\n",
    "\n",
    "# Melting doubled-up geographies\n",
    "ipums_geodf = ipums_geodf.geo2_name.str.split(',').apply(pd.Series) \\\n",
    "    .merge(ipums_geodf, right_index = True, left_index = True) \\\n",
    "    .drop([\"geo2_name\"], axis = 1) \\\n",
    "    .melt(id_vars = [k for k in ipums_geodf.columns if not (type(k)==int)|(k=='geo2_name')], value_name = \"geo2_name\") \\\n",
    "    .drop(\"variable\", axis = 1) \\\n",
    "    .dropna(subset=['geo2_name'])\n",
    "ipums_geodf.geo2_name = ipums_geodf.geo2_name.str.strip()\n",
    "\n",
    "ipums_geodf = ipums_geodf.geo1_name.str.split(',').apply(pd.Series) \\\n",
    "    .merge(ipums_geodf, right_index = True, left_index = True) \\\n",
    "    .drop([\"geo1_name\"], axis = 1) \\\n",
    "    .melt(id_vars = [k for k in ipums_geodf.columns if not (type(k)==int)|(k=='geo1_name')], value_name = \"geo1_name\") \\\n",
    "    .drop(\"variable\", axis = 1) \\\n",
    "    .dropna(subset=['geo1_name'])\n",
    "ipums_geodf.geo1_name = ipums_geodf.geo1_name.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nq_geodf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-4e3165b3647c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnq_geo1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCL_provincia\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnq_geodf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeo1_code\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'nq_geodf' is not defined"
     ]
    }
   ],
   "source": [
    "print(nq_geo1.CL_provincia.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Country-specific pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if country==\"AR\":\n",
    "    # accounting for the CABA mess\n",
    "    ipums_geodf.loc[ipums_geodf.geo1_name==\"City of Buenos Aires\",\n",
    "             'geo1_name'] = 'CABA'\n",
    "    ipums_geodf.loc[(ipums_geodf.geo1_name==\"CABA\"),'geo2_name'] = 'CABA'\n",
    "    caba_codes = ipums_geodf[ipums_geodf.geo1_name==\"CABA\"].geo2_code\n",
    "    # dropping CABA-related duplicates\n",
    "    ipums_geodf.drop_duplicates(subset=[\"geo1_name\",\"geo2_name\"], inplace=True)\n",
    "    # make sure there is only one CABA-code left\n",
    "    #print(len(ipums_geodf[ipums_geodf.geo1_name==\"CABA\"].geo2_code.values[0])==1)\n",
    "    census.loc[census[geo2_ipums].isin(caba_codes),geo2_ipums] = \\\n",
    "    len(census.loc[census[geo2_ipums].isin(caba_codes),geo2_ipums])*[ipums_geodf[ipums_geodf.geo1_name==\"CABA\"].geo2_code[0]]\n",
    "    \n",
    "    # give CABA locations without geo2 a specific code: 999\n",
    "    netquest.loc[netquest[geo2_nq].isna()&(netquest[geo1_nq]==1),geo2_nq] = 999\n",
    "    nq_geodf = netquest.merge(nq_geo2, on=geo2_nq, how='left')\\\n",
    "                       .merge(nq_geo1, on=geo1_nq, how='left')[[geo1_nq,geo1_nq+'_name',geo2_nq,geo2_nq+'_name']]\n",
    "    nq_geodf.columns = ['geo1_code','geo1_name','geo2_code','geo2_name']\n",
    "    nq_geodf.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Fixing CABA not having Dept.\n",
    "    # Where province is CABA and no departamento exists, call these \"CABA\"\n",
    "    nq_geodf.loc[nq_geodf.geo1_name==\"Ciudad Autónoma de Buenos Aires\",\n",
    "                 'geo1_name'] = 'CABA'\n",
    "    nq_geodf = nq_geodf[(nq_geodf['geo1_code'].notna()) & (nq_geodf['geo2_code'].notna())] \n",
    "\n",
    "if (country==\"BR\") or (country==\"CL\"):\n",
    "    nq_geodf = netquest.merge(nq_geo2, on=geo2_nq, how='left')\\\n",
    "                       .merge(nq_geo1, on=geo1_nq, how='left')[[geo1_nq,geo1_nq+'_name',geo2_nq,geo2_nq+'_name']]\n",
    "    nq_geodf.columns = ['geo1_code','geo1_name','geo2_code','geo2_name']\n",
    "\n",
    "    nq_geodf.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geo1_code</th>\n",
       "      <th>geo1_name</th>\n",
       "      <th>geo2_code</th>\n",
       "      <th>geo2_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44.0</td>\n",
       "      <td>Provincia Santiago</td>\n",
       "      <td>296.0</td>\n",
       "      <td>Macul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18.0</td>\n",
       "      <td>Provincia San Felipe de Aconcagua</td>\n",
       "      <td>73.0</td>\n",
       "      <td>San Felipe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>44.0</td>\n",
       "      <td>Provincia Santiago</td>\n",
       "      <td>298.0</td>\n",
       "      <td>Ñuñoa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>22.0</td>\n",
       "      <td>Provincia Talca</td>\n",
       "      <td>112.0</td>\n",
       "      <td>Talca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>19.0</td>\n",
       "      <td>Provincia Cachapoal</td>\n",
       "      <td>95.0</td>\n",
       "      <td>San Vicente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262212</th>\n",
       "      <td>28.0</td>\n",
       "      <td>Provincia Bío-Bío</td>\n",
       "      <td>143.0</td>\n",
       "      <td>Coronel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304280</th>\n",
       "      <td>41.0</td>\n",
       "      <td>Provincia Antártica Chilena</td>\n",
       "      <td>273.0</td>\n",
       "      <td>Antártica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424512</th>\n",
       "      <td>42.0</td>\n",
       "      <td>Provincia Tierra del Fuego</td>\n",
       "      <td>276.0</td>\n",
       "      <td>Timaukel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427644</th>\n",
       "      <td>29.0</td>\n",
       "      <td>Provincia Ñuble</td>\n",
       "      <td>180.0</td>\n",
       "      <td>Chillán Viejo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602640</th>\n",
       "      <td>44.0</td>\n",
       "      <td>Provincia Santiago</td>\n",
       "      <td>311.0</td>\n",
       "      <td>Puente Alto</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>348 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        geo1_code                          geo1_name  geo2_code      geo2_name\n",
       "0            44.0                 Provincia Santiago      296.0          Macul\n",
       "4            18.0  Provincia San Felipe de Aconcagua       73.0     San Felipe\n",
       "8            44.0                 Provincia Santiago      298.0          Ñuñoa\n",
       "12           22.0                    Provincia Talca      112.0          Talca\n",
       "16           19.0                Provincia Cachapoal       95.0    San Vicente\n",
       "...           ...                                ...        ...            ...\n",
       "262212       28.0                  Provincia Bío-Bío      143.0        Coronel\n",
       "304280       41.0        Provincia Antártica Chilena      273.0      Antártica\n",
       "424512       42.0         Provincia Tierra del Fuego      276.0       Timaukel\n",
       "427644       29.0                    Provincia Ñuble      180.0  Chillán Viejo\n",
       "602640       44.0                 Provincia Santiago      311.0    Puente Alto\n",
       "\n",
       "[348 rows x 4 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nq_geodf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Country-independent pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the duplicates in nq_geodf.geo2_code, and keeping the combinations that occur most often (which are quite clearly the good ones).\n",
    "nq_geodf['count'] = nq_geodf.apply(\n",
    "    lambda r: sum((netquest[geo1_nq]==r['geo1_code'])&\n",
    "                  (netquest[geo2_nq]==r['geo2_code'])),\n",
    "                                   axis=1)\n",
    "nq_geodf = nq_geodf.sort_values(['geo2_code','count']) \\\n",
    "    [~(nq_geodf.sort_values(['geo2_code','count']).duplicated('geo2_code',keep='last'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More country-specific processing--manual fixes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "if country==\"AR\":\n",
    "    nq_geodf.loc[nq_geodf.geo2_code==999, 'geo2_name'] = \"CABA\"\n",
    "\n",
    "    ipums_geodf.loc[ipums_geodf.geo1_name.str.contains(\"Buenos Aires province\"),\n",
    "                 'geo1_name'] = \"Buenos Aires\"\n",
    "    \n",
    "    ipums_geodf.loc[ipums_geodf.geo2_name.str.contains(\"Puan\"),\n",
    "                    'geo2_name'] = \"Puán\"\n",
    "    ipums_geodf.loc[(ipums_geodf.geo2_name.str.contains(\"General San Martín\"))\\\n",
    "                    &(ipums_geodf.geo1_name==\"Buenos Aires\"),\n",
    "                    'geo2_name'] = \"Ciudad Libertador San Martín\"\n",
    "    ipums_geodf.loc[(ipums_geodf.geo2_name.str.contains(\"La Capital\"))\\\n",
    "                    &(ipums_geodf.geo1_name==\"San Luis\"),\n",
    "                    'geo2_name'] = \"Juan Martín de Pueyrredón\"\n",
    "    ipums_geodf.loc[(ipums_geodf.geo2_name==\"Maipú\")\\\n",
    "                    &(ipums_geodf.geo2_code==6050),\n",
    "                    'geo2_name'] = \"Marcos Paz\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    ipums_geodf.loc[(ipums_geodf.geo2_name.str.contains(\"Chascomus\")),\n",
    "                    'geo2_name'] = \"Chascomús\"\n",
    "    ipums_geodf.loc[(ipums_geodf.geo2_name.str.contains(\"Jose C. Paz\")),\n",
    "                    'geo2_name'] = \"José C. Paz\"\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name.str.contains(\"Paso de Indios\"))\\\n",
    "                    &(nq_geodf.geo1_name==\"Chubut\"),\n",
    "                    'geo2_name'] = \"Paso de los Indios\"\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name.str.contains(\"Coronel de Marina Leonardo Rosales\"))\\\n",
    "                    &(nq_geodf.geo1_name==\"Buenos Aires\"),\n",
    "                    'geo2_name'] = \"Coronel de Marine L. Rosales\"\n",
    "    ipums_geodf.loc[(ipums_geodf.geo2_name.str.contains(\"Veinticinco de Mayo\"))\\\n",
    "                    &(ipums_geodf.geo1_name==\"Buenos Aires\"),\n",
    "                    'geo2_name'] = \"25 de Mayo\"\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name.str.contains(\"Pueyrredón\"))\\\n",
    "                    &(nq_geodf.geo1_name==\"Buenos Aires\"),\n",
    "                    'geo2_name'] = \"General Pueyrredón\"\n",
    "    \n",
    "elif country==\"BR\":\n",
    "    ipums_geodf.loc[ipums_geodf.geo1_name.str.contains(\"Federal District\"),\n",
    "                'geo1_name'] = \"Distrito Federal\"\n",
    "\n",
    "\n",
    "    ipums_geodf.loc[(ipums_geodf.geo2_name.str.contains(\"Santarém\"))& \\\n",
    "                    (ipums_geodf.geo1_name.str.contains(\"Paraíba\")),\n",
    "                    'geo2_name'] = \"Joca Claudino\"\n",
    "\n",
    "    ipums_geodf.loc[(ipums_geodf.geo2_name.str.contains(\"Presidente Juscelino\"))& \\\n",
    "                    (ipums_geodf.geo1_name.str.contains(\"Rio Grande do Norte\")),\n",
    "                    'geo2_name'] = \"Serra Caiada\"\n",
    "\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name.str.contains(\"Bonfim\"))& \\\n",
    "                    (nq_geodf.geo1_name.str.contains(\"Rio de Janeiro\")),\n",
    "                    'geo1_name'] = \"Minas Gerais\"\n",
    "\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name.str.contains(\"Iracema\"))& \\\n",
    "                    (nq_geodf.geo1_name.str.contains(\"Santa Catarina\")),\n",
    "                    'geo2_name'] = \"Itaiópolis\"\n",
    "\n",
    "\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Pescaria Brava\",'geo2_name'] = 'Laguna'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Balneário Rincão\",'geo2_name'] = 'Içara'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Pinto Bandeira\",'geo2_name'] = 'Bento Gonçalves'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Paraíso das Águas\",'geo2_name'] = 'Água Clara'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Açailândia\",'geo1_name'] = 'Maranhão'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Mariana\",'geo1_name'] = 'Minas Gerais'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Fortaleza\",'geo1_name'] = 'Ceará'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name.isin([\"Leme\",\n",
    "                                          \"São Paulo\",\n",
    "                                          'Itapevi',\n",
    "                                          \"Franca\",\n",
    "                                          \"Presidente Prudente\",\n",
    "                                          \"Suzano\",\n",
    "                                          \"Ourinhos\"\n",
    "                                         ]),'geo1_name'] = 'São Paulo'\n",
    "\n",
    "    nq_geodf.loc[(nq_geodf.geo2_name==\"Praia Grande\") &\\\n",
    "                 ((nq_geodf.geo1_name==\"Acre\")|(nq_geodf.geo1_name==\"Maranhão\")),\n",
    "                 'geo1_name'] = 'São Paulo'\n",
    "\n",
    "    nq_geodf.loc[nq_geodf.geo1_name==\"Brasília\",'geo1_name'] = 'Distrito Federal'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Brasília\",'geo1_name'] = 'Distrito Federal'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Tupanatinga\",'geo1_name'] = 'Pernambuco'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Macaé\",'geo1_name'] = 'Rio de Janeiro'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Queimados\",'geo1_name'] = 'Rio de Janeiro'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"São Miguel do Iguaçu\",'geo1_name'] = 'Paraná'\n",
    "    \n",
    "elif country==\"CL\":\n",
    "    nq_geodf.geo1_name = nq_geodf.geo1_name.str.replace(\"Provincia \",\"\")\n",
    "    nq_geodf.geo1_name = nq_geodf.geo1_name.str.replace(\"de \",\"\")\n",
    "    nq_geodf.loc[nq_geodf.geo1_name==\"Marga Marga\",'geo1_name'] = 'Valparaíso' #(?)\n",
    "    nq_geodf.loc[nq_geodf.geo1_name==\"Tamarugal\",'geo1_name'] = 'Valparaíso'\n",
    "    nq_geodf.loc[nq_geodf.geo1_name==\"Aysen\",'geo1_name'] = 'Aisén'\n",
    "    nq_geodf.loc[nq_geodf.geo1_name==\"Diguillin\",'geo1_name'] = 'Ñuble'\n",
    "    nq_geodf.loc[nq_geodf.geo1_name==\"Itata\",'geo1_name'] = 'Ñuble'\n",
    "    nq_geodf.loc[nq_geodf.geo1_name==\"Punilla\",'geo1_name'] = 'Ñuble'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Alto Hospicio\",'geo2_name'] = 'Iquique'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Aysen\",'geo2_name'] = 'Aisén'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Hualpén\",'geo2_name'] = 'Talcahuano'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Cholchol\",'geo2_name'] = 'Nueva Imperial'\n",
    "    nq_geodf.loc[nq_geodf.geo2_name==\"Alto Bío-Bío\",'geo2_name'] = 'Santa Bárbara'\n",
    "    \n",
    "    ipums_geodf.loc[ipums_geodf.geo2_name.isin([\"Pozo Almonte\",\"Colchane\",'Pica','Huara',\"Camiña\"]),\n",
    "                'geo1_name'] = \"Valparaíso\"\n",
    "    ipums_geodf.loc[ipums_geodf.geo2_name.isin([\"San Fernando\",\"Palmilla\", \"Chimbarongo\", \n",
    "                                            \"Nancagua\",\"Chépica\",\"Pumanque\",\"Placilla\",\n",
    "                                            \"Lolol\", \"Peralillo\", \"Santa Cruz\"\n",
    "                                           ]),\n",
    "               'geo1_name'] = \"Colchagua\"\n",
    "    ipums_geodf.loc[ipums_geodf.geo2_name.isin([\"Lago Ranco\", \"Futrono\", \"Río Bueno\", \n",
    "                                                \"La Unión\"\n",
    "                                           ]),\n",
    "               'geo1_name'] = \"Ranco\"\n",
    "    ipums_geodf.loc[ipums_geodf.geo2_name.str.contains(\"Olmué\"),'geo1_name'] = 'Valparaíso'\n",
    "    ipums_geodf.loc[ipums_geodf.geo2_name==\"Limache\",'geo1_name'] = 'Valparaíso'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are there no more duplicates?\n",
      "True\n",
      "Showing problematic geographies...\n",
      "Empty DataFrame\n",
      "Columns: [geo1_name, geo1_code, geo2_name, geo2_code, geo2_match_name, geo2_match_score]\n",
      "Index: []\n",
      "CL: Decide at what level to cut off matches: 50\n"
     ]
    }
   ],
   "source": [
    "# The fuzzy join\n",
    "nq_geodf[['geo1_match_name','geo1_match_score','geo1_match_index']] \\\n",
    "= fuzzy_join(nq_geodf, ipums_geodf, 'geo1_name')[['name','score','index']]\n",
    "nq_geodf[['geo2_match_name','geo2_match_score','geo2_match_index']] \\\n",
    "= fuzzy_join(nq_geodf, ipums_geodf, 'geo2_name', \n",
    "             subset=['geo1_match_name','geo1_name']\n",
    "            )[['name','score','index']]\n",
    "\n",
    "# confirmation of results\n",
    "print(\"Are there no more duplicates?\")\n",
    "print(\n",
    "    len(nq_geodf[nq_geodf.duplicated('geo2_code', keep=False)].sort_values('geo2_code')\\\n",
    "    [[\"geo1_code\",\"geo2_code\",\n",
    "      \"geo1_name\",\"geo1_match_name\",\n",
    "      \"geo2_name\",\"geo2_match_name\",\n",
    "      \"count\"]]\n",
    "    )==0\n",
    ")      \n",
    "    \n",
    "# determination of cutoff\n",
    "print(\"Showing problematic geographies...\")\n",
    "print(\n",
    "      nq_geodf[(nq_geodf.geo2_match_score<80)][['geo1_name',\n",
    "                                          'geo1_code',\n",
    "#                                         'geo1_match_name',\n",
    "                                        'geo2_name',\n",
    "                                        'geo2_code',\n",
    "                                        'geo2_match_name',\n",
    "                                        'geo2_match_score'\n",
    "                                       ]\n",
    "                ].sort_values('geo1_name'))\n",
    "cutoff = input(country + \": Decide at what level to cut off matches: \")\n",
    "has_ipums_geo = nq_geodf.geo2_match_score>int(cutoff )\n",
    "\n",
    "# attaching geography codes\n",
    "nq_geodf['IPUMS_geo2_code'] = np.nan\n",
    "\n",
    "nq_geodf.loc[has_ipums_geo,'IPUMS_geo2_code'] = nq_geodf[has_ipums_geo]\\\n",
    "                            .geo2_match_index\\\n",
    "                            .astype(int)\\\n",
    "                            .apply(\n",
    "                                lambda i: ipums_geodf.loc[i,'geo2_code']\n",
    "                            .astype(int)\n",
    ")\n",
    "                            \n",
    "# Merge centroids onto NQ geometries:\n",
    "nq_geodf_merged = nq_geodf.merge(geo2_centroids[['ADMIN_NAME','Y','X','IPUM'+year]], \n",
    "               left_on='IPUMS_geo2_code',\n",
    "               right_on=\"IPUM\"+year,\n",
    "               how='left'\n",
    "              ).drop('IPUM'+year, axis=1)\n",
    "#Merge NQ geometries onto NQ data:\n",
    "panel_geo = netquest.merge(nq_geodf_merged[['X','Y','geo2_code']],\n",
    "               left_on=geo2_nq,\n",
    "               right_on='geo2_code',\n",
    "               how='left'\n",
    "              )\n",
    "#Merge census geometries onto census data\n",
    "census_geo = census.merge(geo2_centroids[['ADMIN_NAME','X',\"Y\",'IPUM'+year]],\n",
    "                          left_on = geo2_ipums,\n",
    "                          right_on='IPUM'+year,\n",
    "                          how='left'\n",
    "                         ).drop('IPUM'+year,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if panel_geo.shape[0]!=netquest.shape[0]:\n",
    "    print(\"Problem with panel shape match\")\n",
    "if census_geo.shape[0]!=census.shape[0]:\n",
    "    print(\"Problem with census shape match\")\n",
    "panel_geo.loc[:,[not(\"Unnamed\" in k) for k in panel_geo.columns]].to_csv(panelout)\n",
    "census_geo.loc[:,[not(\"Unnamed\" in k) for k in census_geo.columns]].to_csv(ipumsout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
